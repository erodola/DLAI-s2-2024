{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["i51GvUnwSc0U"],"toc_visible":true,"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"AhTm-wTJNJEJ"},"source":["# Deep Learning & Applied AI\n","\n","# Tutorial 1: Tensor manipulation\n","\n","In this tutorial, we will cover:\n","\n","- PyTorch Tensors: creation, gpu tensors, shape manipulation, indexing\n","\n","Prerequisites:\n","\n","- Python\n","\n","Authors:\n","\n","- Based on an original notebook by Dr. Antonio Norelli (norelli@di.uniroma1.it, now at Oxford University).\n","\n","Course:\n","\n","- Website and notebooks will be available at https://erodola.github.io/DLAI-s2-2024/\n","\n"]},{"cell_type":"markdown","source":["## Welcome to the Deep Learning and Applied AI lab sessions!\n","During the lab sessions, you will be guided through one or more Python notebooks that teach you deep learning tools and provide opportunities to apply what you have learned in class.\n","\n","We encourage you to form small groups of 2-3 people to read and discuss the notebooks together.\n","\n","Run the code and play with it! It is very easy to edit the code locally and make small experiments. Try whatever comes to your mind, this is the best way to learn! Python notebooks are designed to be used in this way, that's why we chose them for the DLAI lab sessions.\n","\n","There will be some exercises, try to do them by yourself, and when everyone in your group has finished, compare the solutions with each other.\n","\n","When something is not clear or you have a question, raise your hand and we will come to you.\n","\n","Some sections in the notebooks are marked with ðŸ“–. This is deepening content for further reading outside of class. You may want to go through it at home or during class if you finish early. (Some sections are \"more optional\" than others, those are marked with more books ðŸ“–ðŸ“–)\n","\n","Let's start!"],"metadata":{"id":"PVQCoYhRizlI"}},{"cell_type":"markdown","metadata":{"id":"MDJI_JVTPMRc"},"source":["## Introduction\n","\n","Many Deep Learning frameworks have emerged for python. Arguably the most notable ones in 2024 are **PyTorch**, **TensorFlow** (with keras frontend) and **Jax**.\n","We will use PyTorch, which is [the leading DL framework](https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/) for research and [continues to gain popularity](https://openai.com/blog/openai-pytorch/).\n","\n","The fundamental data structure of these frameworks is the **tensor**, which is more or less the same everywhere. _A solid understanding of how tensors work is required in deep learning_ and will definitely come in handy in other areas.\n","\n","The first two tutorials will give you solid basics of tensors and operations between tensors."]},{"cell_type":"markdown","metadata":{"id":"5fCEDCU_qrC0"},"source":["## Wait, wait, wait... what is this strange web page with code and text cells all around?\n","\n","It is called Colab, an environment to play with python notebooks directly in your web browser, made by Google. If you never used Colab before, take a look to the following cells, adapted from the official [Colab guide](https://colab.research.google.com/notebooks/welcome.ipynb).\n","\n"]},{"cell_type":"markdown","source":["### Getting started with Colab\n","\n","Colab, or \"Colaboratory\", allows you to write and execute Python in your browser, with\n","- Zero configuration required\n","- Access to GPUs free of charge\n","- Easy sharing"],"metadata":{"id":"IYafT_OGiBAl"}},{"cell_type":"markdown","metadata":{"id":"GJBs_flRovLc"},"source":["The document you are reading is not a static web page, but an interactive environment called a **Colab notebook** that lets you write and execute code.\n","\n","For example, here is a **code cell** with a short Python script that computes a value, stores it in a variable, and prints the result:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJr_9dXGpJ05"},"outputs":[],"source":["seconds_in_a_day = 24 * 60 * 60\n","seconds_in_a_day"]},{"cell_type":"markdown","metadata":{"id":"2fhs6GZ4qFMx"},"source":["To execute the code in the above cell, select it with a click and then either press the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\". To edit the code, just click the cell and start editing.\n","\n","Variables that you define in one cell can later be used in other cells:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-gE-Ez1qtyIA"},"outputs":[],"source":["seconds_in_a_week = 7 * seconds_in_a_day\n","seconds_in_a_week"]},{"cell_type":"markdown","metadata":{"id":"lSrWNr3MuFUS"},"source":["Colab notebooks allow you to combine **executable code** and **rich text** in a single document, along with **images**, **HTML**, **LaTeX** and more. When you create your own Colab notebooks, they are stored in your Google Drive account. You can easily share your Colab notebooks with co-workers or friends, allowing them to comment on your notebooks or even edit them. To learn more, see [Overview of Colab](/notebooks/basic_features_overview.ipynb). To create a new Colab notebook you can use the File menu above, or use the following link: [create a new Colab notebook](http://colab.research.google.com#create=true).\n","\n","Colab notebooks are Jupyter notebooks that are hosted by Colab. To learn more about the Jupyter project, see [jupyter.org](https://www.jupyter.org)."]},{"cell_type":"markdown","metadata":{"id":"_ptdrqwQQAi0"},"source":["## Numpy\n","\n","The adoptive father of Python's deep learning frameworks is Numpy, the historical library which added support for large, multi-dimensional arrays and matrices to Python.\n","\n","As we will see, modern deep learning frameworks (and especially PyTorch) have drawn largely from Numpy's API, while at the same time overcoming its limitations such as the absence of GPU support or automatic differentiation. The student has become the master.\n","\n","![img](https://i.imgur.com/KaUdmee.png)\n","\n","We will mainly use PyTorch tensors for implementing our Deep Learning systems, but knowing how to use Numpy remains very important. Note that:\n","\n","- **Numpy arrays** and **PyTorch tensors** are very similar, most of the features that we will explain for PyTorch tensors apply also to Numpy arrays.\n","- In real DL systems you need to constantly switch between PyTorch and Numpy.\n","\n","If you have prior knowledge of matrix manipulation in Matlab, we recommend the [numpy for Matlab users page](https://numpy.org/doc/stable/user/numpy-for-matlab-users.html) as a useful resource.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"eRzmry5qj8DD"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xsfPgLP9S0gA"},"source":["## PyTorch\n","\n","During the course we'll use and learn many parts of PyTorch API.\n","You should also familiarize with the [PyTorch Documentation](https://pytorch.org/docs/stable/) as it will greatly assist you.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"SWHsmm2OTqZK"},"source":["import torch\n","torch.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7odUeGK8TmVh"},"source":["### **PyTorch Tensor**\n","\n","The ``Tensor`` class is very similar to numpy's ``ndarray`` and provides most of its functionality.\n","\n","\n","However, it also has two important distinctions:\n","\n","- ``Tensor`` supports GPU computations.\n","- ``Tensor`` may store extra information needed for back-propagation:\n","  - The gradient tensor w.r.t. some variable (e.g. the loss)\n","  - A node representing an operation in the computational graph that produced this tensor.\n","\n","We will study back-propagation in a future lecture.\n","\n","Keep in mind:\n","\n","- Usually **tensor operations are not in-place**."]},{"cell_type":"markdown","metadata":{"id":"dkkmkYGDXFFU"},"source":["#### **Tensor instantiation**\n","\n","A tensor represents an n-dimensional grid of values, **all of the same type**."]},{"cell_type":"code","metadata":{"id":"F-i2-H7QU7DH"},"source":["# Basic tensor creation from python lists\n","torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.int32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5LNGnqk3VbkU"},"source":["# Some other tensor construction methods\n","torch.zeros((3,5))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8et3bE93WLBR"},"source":["torch.ones((2,5), dtype=torch.float64)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v2Ehsm4WcML0"},"source":["torch.eye(4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jokiBKanWFIE"},"source":["torch.rand((2,2))  # from which distribution are these random numbers sampled? Check the PyTorch documentation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Pro tip**: Bookmark the [PyTorch docs](https://pytorch.org/docs/stable/)."],"metadata":{"id":"hLz-HYi5rd5-"}},{"cell_type":"code","metadata":{"id":"DHV8h3LVWRaI"},"source":["torch.randint(0, 100, (3,3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zsf9dn4CWl2V"},"source":["t = torch.rand((3, 3))\n","torch.ones_like(t)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KiRfQEztdwqE"},"source":["One can easily convert to/from Numpy tensors:"]},{"cell_type":"code","metadata":{"id":"aI5VsGZJd2rV"},"source":["t = torch.rand((3, 3), dtype=torch.float32)\n","t.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wt4Uy9Bbd81h"},"source":["n = np.random.rand(3,3).astype(np.float16)\n","torch.from_numpy(n)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6FYkcuhs84Sp"},"source":["There are many other functions available to create tensors!"]},{"cell_type":"markdown","metadata":{"id":"RPEmhFYd5TK-"},"source":["> **EXERCISE**\n",">\n","> Create a matrix $M \\in \\mathbb{R}^{3 \\times 3}$ that is filled with 2 along the diagonal and 1 elsewhere, that is:\n",">\n","> $$\n","m_{ij} =\n","\\begin{cases}\n","2 & \\text{if } i = j \\\\\n","1 & \\text{otherwise}\n","\\end{cases}\n","$$"]},{"cell_type":"code","source":["# ðŸ“ write your solution in this cell"],"metadata":{"id":"pAnntNcu9otu"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A2tjmfWbYk9P","cellView":"form"},"source":["# @title ðŸ‘€ Solution\n","\n","\n","torch.ones((3,3)) + torch.eye(3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_c7X85hbWs0c"},"source":["#### **Tensor properties**"]},{"cell_type":"markdown","metadata":{"id":"1XCHWcMlbhH0"},"source":["The **type** of a tensor is the type of each element contained in the tensor:"]},{"cell_type":"code","metadata":{"id":"XP4CyRTmXYpd"},"source":["t = torch.rand((3, 3))\n","t.dtype"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TBoZHIlybZUV"},"source":["\n","The **shape** of a tensor is a tuple of integers giving the size of the tensor along each dimension, e.g. for a matrix $M \\in \\mathbb{R}^{3 \\times 5}$:"]},{"cell_type":"code","metadata":{"id":"PjuSpqbvXbqs"},"source":["t = torch.rand((3,5))\n","t.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UFhAblNmbswv"},"source":["The **device** of a tensor indicates the memory in which the tensor is currently stored: RAM (denoted as ``cpu``) or GPU memory (denoted as ``cuda``)"]},{"cell_type":"code","metadata":{"id":"_4okES1zXcy2"},"source":["t = torch.rand((3,5))\n","t.device"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UM0UCkZ49DPk"},"source":["> **EXERCISE**\n",">\n","> Given a matrix $X \\in \\mathbb{R}^{m \\times n}$, create another matrix $Y \\in \\mathbb{R}^{m \\times 3}$ filled with ones using $X$."]},{"cell_type":"code","metadata":{"id":"_GJinHgLzs11"},"source":["# Exercise variables\n","X = torch.rand(100,42)\n","\n","# Your solution:\n","# Y = ?"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ðŸ‘€ Solution\n","\n","\n","torch.ones((X.shape[0], 3))"],"metadata":{"cellView":"form","id":"A_UxL8zE-n8z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_8Wa3JFCXd7J"},"source":["#### **Using the GPU**\n","\n","Thanks to the explosion of the videogame industry in the last 50 years, the performance of the chips specialized in rendering and processing graphics --known as GPUs-- has dramatically improved.\n","\n","In 2007 NVidia realized the potential of parallel GPU computing outside the videogame world, and released the first version of the CUDA framework, allowing  software developers to use GPUs for general purpose processing.\n","\n","Graphics operations are mostly linear algebra operations, and accelerating them can turn very useful in many other fields.\n","\n","In 2012 Hinton et al. [demonstrated](https://en.wikipedia.org/wiki/AlexNet) the huge potential of GPUs in training deep neural networks, starting *de facto* the glorious days of deep learning."]},{"cell_type":"code","metadata":{"id":"bgZST3s_CaLm","cellView":"form"},"source":["# @title GPUs are very powerful\n","\n","from IPython.display import YouTubeVideo, HTML, display\n","\n","display(YouTubeVideo('-P28LKWTzrI?t=14'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gbIogawQYBvW"},"source":["# Check if the GPU is available\n","torch.cuda.is_available()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TVYbgoigX_PL"},"source":["# If available use the GPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yhHVeolzWZQn"},"source":["t = torch.rand((3,3))\n","t = t.to(device)  # Note that we are assigning back to t, otherwise t won't be updated!\n","t"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Al_WAx4RWDy2"},"source":["# Construct tensors directly on the GPU memory\n","t = torch.ones((5, 5), device='cuda')\n","t"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nuynABfrVZmD"},"source":["t = torch.rand((3,3))\n","\n","# Other shortcuts to transfer tensors between devices\n","\n","# Be careful of hardcoded cuda calls: the code will not run if a GPU is not available\n","t = t.cuda()\n","t"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JkOX8icwVXPK"},"source":["t = t.cpu()\n","t"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mj0gWQ-FgsZ2"},"source":["# Utility function to print tensors nicely. We will use this all the time.\n","\n","from typing import Union, Sequence\n","\n","def print_arr(\n","    *arr: Sequence[Union[torch.Tensor, np.ndarray]], prefix: str = \"\"\n",") -> None:\n","    \"\"\"\n","    Pretty print tensors, together with their shape and type\n","\n","    :param arr: one or more tensors\n","    :param prefix: prefix to use when printing the tensors\n","    \"\"\"\n","    print(\n","        \"\\n\\n\".join(\n","            f\"{prefix}{str(x)} <shape: {x.shape}> <dtype: {x.dtype}>\" for x in arr\n","        )\n","    )\n","\n","t = torch.rand((3,3), dtype=torch.float32)\n","print_arr(t, prefix='My tensor = ')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1g8l8j4deyZ2"},"source":["#### ðŸ“– **Tensor rank**\n","\n","In Numpy and PyTorch, the **rank of a tensor** denotes the number of dimensions. For example, any matrix is a tensor of rank 2.\n","\n","Don't confuse this with the rank of a matrix, which has a completely different meaning in linear algebra!"]},{"cell_type":"markdown","metadata":{"id":"pF8r6t6VTbrf"},"source":["- **rank-0** tensors are just scalars"]},{"cell_type":"code","metadata":{"id":"lfSjfJ7bTHlZ"},"source":["t0 = torch.tensor(3, dtype=torch.double)\n","\n","print_arr(t0)  # notice torch.Size in the printed output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4O2GLaL0SxHn"},"source":["item = t0.item()  # convert the tensor scalar to a python base type\n","item, type(item)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aPcQfgGbaEqv"},"source":["# Be careful, a non-scalar tensor cannot be converted with an .item() call\n","try:\n","  x = torch.ones(3).item()\n","except RuntimeError as e:\n","  print('Error:', e)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"El2afq3-Sujl"},"source":["- **rank-1** tensors are sequences of numbers. A sequence of length ``n`` has the shape ``(n,)``"]},{"cell_type":"code","metadata":{"id":"Twmbub4VSrvt"},"source":["# A rank-1 tensor\n","t1 = torch.tensor([1, 2, 3])\n","\n","print_arr(t1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3LA-5C4kSHXv"},"source":["# A rank-1 tensor with a single scalar\n","print_arr(torch.tensor([42]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["PyTorch and NumPy are smart: if a tensor is not rank-0 but can be converted to a rank-0 tensor, then the .item() will work.\n","\n","This operation is called **broadcasting**, we will see it in detail in the next notebooks."],"metadata":{"id":"PEGUO2rcxtwi"}},{"cell_type":"code","metadata":{"id":"7xuBu8bKcbgl"},"source":["# A rank-1 tensor with a single element can be converted to a rank-0 tensor\n","torch.tensor([42]).item()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **NOTE**\n",">\n","> Don't be too hopeful about mapping Pytorch concepts onto mathematical concepts.\n",">\n","> There isnâ€™t a distinction in Pytorch between row vectors and column vectors: both are just rank-1 tensors!"],"metadata":{"id":"gU-LyxtryotQ"}},{"cell_type":"markdown","metadata":{"id":"Lj0NVuY3Rzp0"},"source":["- **rank-2** tensors have the shape ``(n, m)``"]},{"cell_type":"code","metadata":{"id":"wui6OfMFPdnM"},"source":["t2 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","\n","print_arr(t2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Again, it doesn't make sense to talk about the \"rows\" and \"columns\" of rank-2 tensors."],"metadata":{"id":"Z_W9Pi50zMD-"}},{"cell_type":"code","source":["# element (i,j) of a rank-2 tensor just means the j-th element of the i-th rank-1 tensor\n","t2[1, 2].item()"],"metadata":{"id":"o6dSV3eWzvm-"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_hBNed-yOmmQ"},"source":["# To mimick the notion of a column vector from linear algebra, we can use a rank-2 tensor\n","t_col = t1.reshape(-1, 1)\n","\n","print_arr(t_col)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qad_RigUjEtA"},"source":["# ...and similarly for row vectors\n","t_row = t1.reshape(1, -1)\n","\n","print_arr(t_row)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This way, we the standard matrix product will work only if the dimensions match\n","\n","t = torch.ones(10)  # rank-1 tensor\n","\n","t_row = t.reshape(1, -1)  # rank-2 'row vector'\n","t_col = t.reshape(-1, 1)  # rank-2 'column vector'\n","\n","_ = t_row @ torch.ones(10, 3)  # does not work with t_col\n","_ = torch.ones(3, 10) @ t_col  # does not work with t_row\n","\n","# Notice that the matrix product still does the right thing if we multiply by the rank-1 tensor\n","\n","_ = t @ torch.ones(10, 3)\n","_ = torch.ones(3, 10) @ t"],"metadata":{"id":"sOdxEQFwBpgy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BeGPcSYMOlv0"},"source":["- **rank-k** tensors have a shape of $(n_1, \\dots, n_k)$"]},{"cell_type":"code","metadata":{"id":"hg5bbeCIjuL_"},"source":["print_arr(torch.zeros((2, 3, 4)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3O68PwBGOjpe"},"source":["print_arr(torch.ones((2, 2, 2, 2)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xkugOzEKi3wk"},"source":["> **EXERCISE**\n",">\n","> Build a tensor $X \\in \\mathbb{R}^{k \\times k}$ filled with zeros and the sequence $[0, ..., k-1]$ along the diagonal"]},{"cell_type":"code","metadata":{"id":"v_8mTH8pdzHz"},"source":["# your solution\n","k = 12"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ðŸ‘€ Solution\n","\n","\n","k = 12\n","X = torch.diag(torch.arange(k))\n","print_arr(X)"],"metadata":{"id":"38L1P8aCCXed","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Dixdu5yjZ0g"},"source":["> **EXERCISE**\n",">\n","> What is the shape of the following tensor?\n",">\n","> ```python\n","> torch.tensor(\n",">     [\n",">         [[1.0, 1.0, 1.0],\n",">          [1.0, 1.0, 1.0]],\n",">\n",">         [[1.0, 1.0, 1.0],\n",">          [1.0, 1.0, 1.0]],\n",">\n",">         [[1.0, 1.0, 1.0],\n",">          [1.0, 1.0, 1.0]],\n",">\n",">         [[1.0, 1.0, 1.0],\n",">          [1.0, 1.0, 1.0]],\n",">     ]\n","> )\n","> ```\n","\n"]},{"cell_type":"code","metadata":{"id":"PXIhMoIO1DJA"},"source":["# Think about it, then confirm your answer by writing code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tfGGF8mmSJkq"},"source":["### **Changing and adding dimensions**\n","\n","PyTorch provides several functions to manipulate tensor shapes\n"]},{"cell_type":"markdown","metadata":{"id":"i51GvUnwSc0U"},"source":["#### **Transpose dimension**"]},{"cell_type":"code","metadata":{"id":"g1kH1K7XS6KS"},"source":["a = torch.ones((3, 5))\n","a[0, -1] = 0  # index -1 denotes the last element, as in common python indexing\n","print(\"a: \")\n","print_arr(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DUXAlWb7fJn9"},"source":["a.T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VqEfj7tuTAqK"},"source":["a.transpose(1, 0)  # Swap dimension 1 and 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9wvUahrTS9uJ"},"source":["torch.einsum('ij -> ji', a)  # transpose using Einstein notation\n","\n","# In the next notebook we will explain the Einstein notation in detail"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9nXnyRKSfPSw"},"source":["#### ðŸ“– Transpose in k-dimensions and in Numpy\n"]},{"cell_type":"code","metadata":{"id":"Sww5AtujTPiJ"},"source":["a = torch.ones((2, 3, 6))\n","a[1, 2, 4] = 42\n","print_arr(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bFezEV5uTc1W"},"source":["a.transpose(2, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pIYTVMyRUC1e"},"source":["torch.einsum('ijk->ikj', a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Shortcuts are handy, but your code becomes less readable.\n","Most of the time readability is the most important goal to aim for!\n","\n","What do you think `a.T` will do to our rank-3 tensor? Once you have your hypothesis, test it here:"],"metadata":{"id":"iF0q66cg-M6c"}},{"cell_type":"code","source":["a.T"],"metadata":{"id":"hgp1QJWK-cX3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is confusing even after reading the docs!\n","\n","Lesson learned: Prefer readable code to short code."],"metadata":{"id":"-7zn8PWh-toL"}},{"cell_type":"markdown","metadata":{"id":"a9Jshcp9Ul5O"},"source":["> **NOTE**\n",">\n","> In Numpy the transpose function is different!\n",">\n","> PyTorch:\n","> `torch.transpose(input, dim0, dim1) â†’ Tensor`\n",">\n","> NumPy:\n","> `numpy.transpose(a, axes=None) -> numpy.ndarray`\n",">\n","> Compare the docs from [numpy](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html) and [pytorch](https://pytorch.org/docs/stable/generated/torch.transpose.html)\n",">\n","> In PyTorch the transpose swaps two dimensions. In NumPy you can specify a complete mapping to change all the dimensions."]},{"cell_type":"code","metadata":{"id":"K9ktDbO5Uswj"},"source":["a = np.arange(10).reshape(2, 5)\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h0QGO8d_hFmQ"},"source":["a.transpose(1, 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8LLh0Qd7U2xT"},"source":["a.transpose(0, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eu4YJcgAhAds"},"source":["torch.from_numpy(a).transpose(0, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fsU83yRwU9VW"},"source":["# The einsum is cross platform. It works with consistent semantics\n","# pretty much everywhere: PyTorch, NumPy, TensorFlow, Jax, ...\n","# We will see the power of einsum in the next lab\n","np.einsum('ij -> ji', a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RWaicg6uVIXQ"},"source":["#### **Reshape**\n","\n","Another important feature is **reshaping** a tensor into different dimensions\n","\n","- We need to make sure to **preserve the same number of elements**.\n","- `-1` in one of the dimensions means **\"figure it out\"**.\n"]},{"cell_type":"markdown","metadata":{"id":"L5uTNjKI15kx"},"source":["âŒâŒâŒ Pay attention that **transposing and reshaping are two fundamentally different operations**:"]},{"cell_type":"code","metadata":{"id":"W01aXqIZwfCu"},"source":["a = torch.arange(12).reshape(3,4 )\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c67xp19WwmGP"},"source":["# The classical transpose\n","a.t()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EenpJs6gwoLd"},"source":["# Reshape into the transpose shape\n","a.reshape(4, 3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### ðŸ“– **What is `reshape` really doing?**\n"],"metadata":{"id":"VoIngdEzwQNY"}},{"cell_type":"markdown","metadata":{"id":"wJPn4ArY2Q6E"},"source":["\n","Think of the `reshape` operation as unrolling the tensor **row-wise**, to obtain a rank-1 tensor *(matlab users: matlab unrolls **column-wise**, pay attention when converting code!)*. Then it stores the values in this tensor following the specified dimensions.\n","\n","```python\n","tensor([[ 0,  1,  2,  3],\n","        [ 4,  5,  6,  7],\n","        [ 8,  9, 10, 11]])\n","```\n","$-$ unrolling $ \\to $\n","\n","```python\n","tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n","```\n","\n","Then, reading the target shape from right to left, organize the values into the dimensions:\n","\n","- e.g. reshape into `[4, 3]`:\n","\n","```python\n","tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n","```\n","\n","$-$ organize in groups of $3$ $ \\to $\n","\n","```python\n","tensor([[0,  1,  2],  [3,  4,  5],  [6,  7,  8],  [9, 10, 11]])\n","```\n","\n","$-$ organize in groups of $4$ $ \\to $\n","\n","```python\n","tensor([[ 0,  1,  2],\n","        [ 3,  4,  5],\n","        [ 6,  7,  8],\n","        [ 9, 10, 11]])\n","\n","# same shape of corresponding transpose, but the values are stored differently!\n","```\n","\n","- e.g. reshape into `[2, 2, 3]`:\n","\n","```python\n","tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n","```\n","\n","$-$ organize in groups of $3$ $ \\to $\n","\n","```python\n","tensor([[0,  1,  2],  [3,  4,  5],  [6,  7,  8],  [9, 10, 11]])\n","```\n","\n","$-$ organize in groups of $2$ $ \\to $\n","\n","```python\n","tensor([[[0,  1,  2],  [3,  4,  5]],  [[6,  7,  8],  [9, 10, 11]]])\n","```\n","\n","$-$ organize in groups of $2$ $ \\to $\n","\n","```python\n","tensor([[[ 0,  1,  2],\n","         [ 3,  4,  5]],\n","\n","        [[ 6,  7,  8],\n","         [ 9, 10, 11]]])\n","```"]},{"cell_type":"code","metadata":{"id":"U3U_yh07Vf7m"},"source":["a = torch.arange(12)\n","print_arr(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_mqUJve9VjUQ"},"source":["a.reshape(6, 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i-qtMeW0VnTu"},"source":["a.reshape(2, 6)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S9miZ3XUqxvc"},"source":["a.reshape(2, 2, 3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1rUiVJQgWibl"},"source":["try:\n","  a.reshape(5, -1)\n","except RuntimeError as e:\n","  print('Error:', e)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9STd1HDBWkym"},"source":["a.reshape(1, -1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-zjpBpctWr-D"},"source":["a.reshape(-1, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qWpSkDFAWo_Q"},"source":["a.reshape(-1)  # we are flattening the rank-k tensor into a rank-1 tensor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RS6Bg3gg-X-F"},"source":["> **NOTE**\n",">\n","> We can add or remove dimensions of size `1` using `torch.unsqueeze` or `torch.squeeze`"]},{"cell_type":"code","metadata":{"id":"WuRS4c8HmUNB"},"source":["a"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a.shape"],"metadata":{"id":"Gz9-eqQtAR_J"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F2Z05GiFmUpt"},"source":["a.unsqueeze(0).shape  # adds a new dimension at the beginning"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UXZUg0P_mcha"},"source":["a.unsqueeze(-1).shape  # adds a new dimension at the end"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OYWdXFvqrbqS"},"source":["> **NOTE**\n",">\n","> Often the reshape does not require a physical copy of the data, but just a logical\n","> reorganization.\n",">\n","> If you are curious about the NumPy/PyTorch tensor internals, a good starting point to learn about *strides* is this [SO answer](https://stackoverflow.com/questions/53097952/how-to-understand-numpy-strides-for-layman).\n","> tldr: often you can reshape tensors by changing only its strides and shape. The strides  are the byte-separation between consecutive items for each dimension.\n",">\n","> To be sure to obtain a *view* of the tensor, that shares the same underlying data, you can use the `torch.view` method.\n","> Its semantics is similar to `reshape`, but it works only on [`contiguous` tensors](https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/2) and it guarantees that no copy will be performed."]},{"cell_type":"markdown","metadata":{"id":"VYocF4AUBJN4"},"source":["> **EXERCISE**\n",">\n","> Given a sequence of increasing numbers from `0` to `9`, defined as:\n",">\n","> ```python\n","> a = torch.arange(10)\n","> ```\n",">\n","> Use only the `reshape` and `transpose` functions to obtain the following tensor from `a`:\n",">\n","> ```python\n","> tensor([0, 2, 4, 6, 8, 1, 3, 5, 7, 9])\n","> ```"]},{"cell_type":"code","metadata":{"id":"leLESEEVD-Eq"},"source":["# Your solution\n","\n","a = torch.arange(10)\n","print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ðŸ‘€ Solution\n","\n","\n","a.reshape(5, 2).transpose(0, 1).reshape(1, -1).squeeze()"],"metadata":{"cellView":"form","id":"3KUBAb3mCYOS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zjvDvo4yXAk-"},"source":["#### **Concatenation**\n","\n","PyTorch provides many functions to manipulate tensors.\n","Two of the most common functions are:\n","\n","- `torch.stack`: Adds a **new** dimension, and concatenates the given tensors along that dimension.\n","- `torch.cat`: Concatenates the given tensors along one of the **existing** dimensions."]},{"cell_type":"code","metadata":{"id":"Oq1IMbFvXLPk"},"source":["a = torch.arange(12).reshape(3, 4)\n","b = torch.arange(12).reshape(3, 4) + 100\n","print_arr(a, b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out = torch.stack((a, b), dim=0)\n","print_arr(out)"],"metadata":{"id":"idTJRE6oDRpi"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EsyXtEm4XRf1"},"source":["out = torch.cat((a, b), dim=0)\n","print_arr(out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out = torch.cat((a, b), dim=1)\n","print_arr(out)"],"metadata":{"id":"upRrgqeZILbb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qHQFWwzCn8k7"},"source":["> **EXERCISE**\n",">\n","> Given a tensor $X \\in \\mathbb{R}^{3 \\times 1920 \\times 5 \\times 1080}$ reorganize it in order to obtain a tensor $Y \\in \\mathbb{R}^{5 \\times 1920 \\times 1080 \\times 3}$\n",">\n","> Think of $X$ as a tensor that represents $5$ RGB images of size $1080\\times 1920$. Your goal is to reorganize this tensor in a sensible (and usable) way.\n"]},{"cell_type":"code","metadata":{"id":"PrMb_KB_ogMs"},"source":["a = torch.rand(3, 1920, 5, 1080)\n","a.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Your solution\n"],"metadata":{"id":"b2twlGrOJ0VO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ðŸ‘€ Solution\n","\n","a.transpose(0, 2).transpose(2, 3).shape\n","\n","# Equivalent solution\n","a.permute(2, 1, 3, 0).shape"],"metadata":{"cellView":"form","id":"nLpeXAQ1I1zR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QPHnbIH_rn_C"},"source":["### **Tensor indexing**\n","\n","PyTorch offers several ways to index tensors\n"]},{"cell_type":"markdown","metadata":{"id":"U68ToAl1r_dG"},"source":["#### **Standard indexing**\n","\n","As a standard Python list, PyTorch tensors support the python indexing conventions:"]},{"cell_type":"code","metadata":{"id":"PWzGd3vYsb8e"},"source":["a = torch.arange(10)\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vTn3_9F3tAVl"},"source":["print(a[0])  # first element\n","print(a[5])  # sixth element"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vDGstnvrs6wf"},"source":["print(a[-1])  # last element\n","print(a[-2])  # second last element"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t_uRKJqor2Ty"},"source":["#### **Multidimensional indexing**\n","\n","Since tensors may be multidimensional, you can specify **one index for each dimension**:"]},{"cell_type":"code","metadata":{"id":"4oSg1hGQtyT8"},"source":["a = torch.arange(10).reshape(2, 5)\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QfPPgXguqeT9"},"source":["a[1, 3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p_d7tUsVuKNM"},"source":["a[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6zErRdLjud3S"},"source":["a[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j40QvNuLugWX"},"source":["a[0, -1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **EXERCISE**\n",">\n","> What element is at position `a[1, -1]`?"],"metadata":{"id":"ei6RS_wkKJp6"}},{"cell_type":"markdown","metadata":{"id":"XY76VPqCuhc1"},"source":["#### **Slicing**\n","\n","Similar to Python sequences and Numpy arrays, PyTorch tensors can be easily sliced using the slice notation:\n","\n","```python\n","a[start:stop]  # items from start to stop-1 (i.e. the last element is excluded)\n","a[start:]      # items from start through the rest of the array\n","a[:stop]       # items from the beginning through stop-1\n","a[:]           # a shallow copy of the whole array\n","```\n","\n","There is also an optional step value, which can be used with any of the above:\n","\n","```python\n","a[start:stop:step] # from start to at most stop-1, by step\n","```"]},{"cell_type":"code","metadata":{"id":"ku-xaZ7LvjR5"},"source":["# Sum with scalar acts element-wise\n","a = torch.arange(10) + 10\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Take the elements in positions 5..6\n","a[5:7]"],"metadata":{"id":"7TRwjulxKo_E"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aut0rgIEwIG_"},"source":["# Take the last 5 elements\n","a[-5:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UZ3vLffywvQt"},"source":["# Select every element having an even index\n","a[::2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lMgTjK6yyRCl"},"source":["With multidimensional tensors we can perform **multidimensional slicing**:"]},{"cell_type":"code","metadata":{"id":"ZJYcchmxykd6"},"source":["a = torch.arange(10).reshape(2, 5)\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gkhWYbVHyq-H"},"source":["# Take the second column\n","a[:, 1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5PzkxXpdyMoL"},"source":["# Take the last column\n","a[:, -1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7q0GVL_YxVRK"},"source":["# Take a slice from the last row\n","a[-1, -3:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z6oeDEvEzvhQ"},"source":["You can **assign** to sliced tensors, therefore *modifying the original tensor*.\n","\n","This means that sliced tensors are **shallow copies**: the resulting tensors **share the underlying data** with the original tensor."]},{"cell_type":"code","metadata":{"id":"R2XdMXUUzLFV"},"source":["a = torch.arange(10).reshape(2, 5)\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aiM4-1Je1C3y"},"source":["b = a[0:2, 1:3]\n","b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Evbxee1A1GFo"},"source":["b[-1, :] = -999\n","b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"44A4NAYewvYd"},"source":["# The original tensor has been modified\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"chqxaQU63oxh"},"source":["a[-1, -1] = -1\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y_yFS2q51zdn"},"source":["> **NOTE**\n",">\n","> Indexing with **integers yields lower rank tensors**\n",">\n","> Integer indexing simply means we don't use slices (:) or boolean masks for indexing."]},{"cell_type":"code","metadata":{"id":"CqLGPTtlu22k"},"source":["a = torch.arange(12).reshape(3, 4)\n","print_arr(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u2vHVDozvCag"},"source":["# Rank-1 view of the second row of a\n","row_r1 = a[1, :]\n","print_arr(row_r1)  # notice the size of the resulting tensor, which is now lower than the original tensor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CTsk6eG325Ib"},"source":["# Rank-2 view of the second row of a\n","row_r2 = a[1:2, :]\n","print_arr(row_r2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zUVPrn7l3A5j"},"source":["# Rank-2 view of the second row of a\n","row_r3 = a[[1], :]\n","print_arr(row_r3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TL0qs9Gw3Mfu"},"source":["# Same with the columns\n","print_arr(a[:, 1])\n","print_arr(a[:, [1]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TjIcC1HqwvU1"},"source":["#### ðŸ“–ðŸ“– **Slice Object**\n","\n","The **slice syntax** is just a shortand.\n","\n","In Python everything is an object, even a ``slice``.\n","It is possible to explicitly create a ``Slice`` object and reuse it to **index multiple tensors in the same way**:\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"3D8qk7q0xUAb"},"source":["# The signature follows the same pattern as above: (begin, end, step)\n","\n","s1 = slice(3)  # equivalent to the slice [:3]\n","s1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fMqp5ZCEp8uu"},"source":["type(s1)  # Slice is a python built-in type!"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JIxqkduyp470"},"source":["out = a[s1]  # equivalent to a[:3]\n","\n","print_arr(a, out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JWr9dLVqxziR"},"source":["mystring = 'this is just a string'\n","mystring[s1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RNehWUQn2t4H"},"source":["s2 = slice(None, None, -1)  # equivalent to [::-1]\n","mystring[s2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XvkH5XOCyJQx"},"source":["try:\n","  a[s2]  # PyTorch currently does not support negative steps\n","except ValueError as e:\n","  print('Error:', e)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UUsBgzbl33Qc"},"source":["#### ðŸ“– **Integer array indexing**\n","\n","When we use slices (:), the resulting tensor view will always be a subarray of the original tensor.\n","\n","In contrast, if we index with integers only, we can construct arbitrary tensors using the data from another tensor."]},{"cell_type":"code","metadata":{"id":"8yJ7eft44Shf"},"source":["a = torch.arange(1, 7).reshape(3, 2)\n","print_arr(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YDGTkvIQ4qaO"},"source":["# Example of integer array indexing\n","# The returned array will have shape (3,)\n","b = a[[0, 1, 2], [0, 1, 0]]\n","print_arr(b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8AjjBKF4nos"},"source":["# The above is equivalent to:\n","v1, v2, v3 = a[0, 0], a[1, 1], a[2, 0]\n","b = torch.tensor([v1, v2, v3])\n","print_arr(b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WnVB223B5Nf9"},"source":["# You can re-use the same element of the source tensor multiple times!\n","print_arr(a[[0, 0], [1, 1]])\n","print_arr(torch.tensor([a[0, 1], a[0, 1]]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NG8PUPue5ntQ"},"source":["# You can use another tensor to perform the indexing,\n","# as long as they have dtype=torch.int64 (synonym for torch.long)\n","i = torch.ones(3, dtype=torch.int64)\n","i"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yp--PCJN5uvA"},"source":["j = torch.tensor([0, 1, 0])\n","j"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RrlnZbw25w3H"},"source":["out = a[i, j]\n","\n","print_arr(a, out)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gzr5hn34sjko"},"source":["> **EXERCISE**\n",">\n","> Using a single assignment, change the elements of a tensor $X \\in \\mathbb{R}^{4 \\times 3}$ as follows:\n",">\n","> `X[0,2] = -1`\n",">\n","> `X[1,1] = 0`\n",">\n","> `X[2,0] = 1`\n",">\n","> `X[3,1] = 2`\n","\n"]},{"cell_type":"code","metadata":{"id":"F_3bZ5xHwmGb"},"source":["# Mutate one element from each row of a matrix\n","a = torch.arange(12).reshape(4, 3)\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ðŸ‘€ Solution\n","\n","\n","a[[0, 1, 2, 3], [2, 1, 0, 1]] = torch.tensor([-1, 0, 1, 2])\n","a"],"metadata":{"cellView":"form","id":"A62V7F_PI_Cn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"544F9pAf-qRY"},"source":["> âŒâŒâŒ **NOTE**\n",">\n","> **Slice indexing vs Array indexing**\n",">\n","> Be careful, since slice indexing and array indexing are different operations!"]},{"cell_type":"code","metadata":{"id":"b-GYMhxA-tOY"},"source":["a = torch.arange(16).reshape(4, 4)\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Piy-JUdv-x0_"},"source":["a[0:3, 0:3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fXJJAq9G-0rX"},"source":["a[[0, 1, 2], [0, 1, 2]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OKjSW5ePxzLW"},"source":["a[torch.arange(0,3), torch.arange(0,3)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hkmiQgQT_k82"},"source":["a[0:5:2, 0:5:2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MUf_I1xjx8f9"},"source":["# With *slice indexing* you return a sub-tensor."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QVMo8E2W_wAw"},"source":["#### **Boolean array indexing**\n","\n","This type of indexing is used to select the elements of a tensor that satisfy some condition (similar to MATLAB's logical indexing):"]},{"cell_type":"code","metadata":{"id":"OFIFupWJAnI9"},"source":["a = torch.arange(6).reshape(3, 2)\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jMaNbpVPAtMm"},"source":["bool_idx = (a > 2)\n","bool_idx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XbYWFo8TArod"},"source":["a[bool_idx]  # remember that NumPy and PyTorch unroll row-wise and not column-wise like Matlab"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kzVyXjJgBMKa"},"source":["If you want to know more about indexing in PyTorch and Numpy read the [docs](https://numpy.org/doc/stable/user/basics.indexing.html#basics-indexing)"]},{"cell_type":"markdown","source":["##### ðŸ“–ðŸ“– **Graph use case**\n","\n","Suppose you have a weighted adjacency matrix for a directed graph. We want to obtain a list of edges with weight greater than 0.5.\n","\n","How can we do that?\n","\n","\n","\n"],"metadata":{"id":"cZUsTLVGPXmc"}},{"cell_type":"code","source":["# Let's define a random adjacency\n","a = torch.randint(2, (5, 5)).bool()\n","adj_matrix = ((a + a.T) > 0.5) * torch.rand_like(a, dtype=torch.float)\n","adj_matrix"],"metadata":{"id":"zReQ6XJdQHfY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The list of edges\n","(adj_matrix > 0.5).nonzero()"],"metadata":{"id":"dmHUoUUBP7AU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The asssociated weights for each edge\n","adj_matrix[adj_matrix > 0.5]"],"metadata":{"id":"6zyU9SBOR-JQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exercises"],"metadata":{"id":"mSAHCIYePoB5"}},{"cell_type":"markdown","metadata":{"id":"fbUID2DlLuhq"},"source":["> **EXERCISE**\n",">\n","> Build a 3D tensor in $X \\in \\mathbb{R}^{3 \\times 3 \\times 3}$ that has ones along the 3D-diagonal and zeros elsewhere, i.e. a 3D identity."]},{"cell_type":"code","source":["# Write here your solution\n","# X = ?"],"metadata":{"id":"trx7jDTuLe2D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ðŸ‘€ Solution\n","\n","\n","X = torch.zeros(3, 3, 3)\n","X[torch.arange(3), torch.arange(3), torch.arange(3)] = 1\n","X"],"metadata":{"cellView":"form","id":"G2lUX3SbLS3W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5uLHb9jWM0jM"},"source":["> **EXERCISE**\n",">\n","> You are given a 3D tensor $X \\in \\mathbb{R}^{w \\times h \\times 3}$ representing a $w \\times h$ image with `(r, g, b)` color channels. Assume that colors take values in $[0, 1]$.\n",">\n","> Color the image $X$ completely by red, i.e. `(1, 0, 0)` in the `(r, g, b)` format."]},{"cell_type":"code","metadata":{"id":"g_HG8I5zNnpJ"},"source":["# Create and visualize a black image\n","x = torch.zeros(100, 200, 3)\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","img = plt.imshow(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gqqm2zZUybB7"},"source":["# Write here your solution"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ðŸ‘€ Solution\n","\n","\n","x[:, :, 0] = 1\n","img = plt.imshow(x)"],"metadata":{"cellView":"form","id":"Iu4XPwmkNEtg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ps_j8BUcQmWu"},"source":["> **EXERCISE**\n",">\n","> You are given the GitHub logo $X \\in \\mathbb{R}^{560 \\times 560}$.  Assume the logo is in gray scale, with the color $c \\in [0, 1]$ (remember 0 $\\to$ black).\n",">\n","> 1. Change the black-ish color into light gray: $0.8$.\n","> 2. Then draw a diagonal and anti-diagonal black line (i.e. an X) on the new image, to mark that the new logo is wrong."]},{"cell_type":"code","metadata":{"id":"X0082ZNOP-EK"},"source":["from skimage import io\n","\n","image = io.imread('https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png', as_gray=True)\n","_ = plt.imshow(image, cmap='gray', vmin=0, vmax=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"thBZCe_3QyxM"},"source":["# Change the black into light-gray\n","X = torch.from_numpy(image.copy())  # PyTorch CPU and Numpy share the memory!\n","# # ?\n","\n","_ = plt.imshow(X, cmap='gray', vmin=0, vmax=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DvjjGYmkX8m7"},"source":["# # Mark the new image as wrong with a big black X\n","# # ?\n","\n","_ = plt.imshow(X, cmap='gray', vmin=0, vmax=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ðŸ‘€ Solution\n","\n","\n","X[X < 1] = 0.8\n","X[torch.arange(X.shape[0]), torch.arange(X.shape[1])] = 0\n","X[torch.arange(X.shape[0] - 1, -1, -1), torch.arange(X.shape[1])] = 0\n","\n","_ = plt.imshow(X, cmap='gray', vmin=0, vmax=1)"],"metadata":{"cellView":"form","id":"IRYlOz1CO6qZ"},"execution_count":null,"outputs":[]}]}