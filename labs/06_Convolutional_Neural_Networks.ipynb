{"cells":[{"cell_type":"markdown","metadata":{"id":"4C5Ct9yoZKYa"},"source":["# Deep Learning & Applied AI\n","\n","We reccomend to go through the notebook using Google Colaboratory.\n","\n","# Tutorial 6: Convolutional neural networks\n","\n","In this tutorial, we will cover:\n","\n","- Convolutional neural networks\n","- Residual networks\n","\n","Based on original material by Dr. Luca Moschella, Dr. Antonio Norelli and Dr. Marco Fumero.\n","\n","Course:\n","\n","- Website and notebooks will be available at https://github.com/erodola/DLAI-s2-2024/"]},{"cell_type":"markdown","metadata":{"id":"UzZFFAD7ujN4"},"source":["##Import dependencies (run the following cells)"]},{"cell_type":"code","source":["!pip install plotly==5.3.1\n","!pip install numpy==1.23.0"],"metadata":{"id":"HN-xxGnVId9t"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pRePt-K1_yw9"},"outputs":[],"source":["# @title import dependencies\n","\n","from typing import Mapping, Union, Optional\n","\n","import numpy as np\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import plotly.graph_objects as go\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import os\n","from tqdm.notebook import tqdm\n","\n","from __future__ import print_function, division"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2tGN_bJOcfd3"},"outputs":[],"source":["# @title reproducibility stuff\n","\n","import random\n","torch.manual_seed(42)\n","np.random.seed(42)\n","random.seed(0)\n","\n","torch.cuda.manual_seed(0)\n","torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n","torch.backends.cudnn.benchmark = False"]},{"cell_type":"markdown","metadata":{"id":"9IwQo2VY2gaZ"},"source":["##Intro\n","\n","Today we start by going back to March 2016.\n","\n","Do you remember this match?\n","\n","![AlphaGo](https://mediad.publicbroadcasting.net/p/shared/npr/201805/470700854.jpg)\n","\n","\n","\n","For the first time an artificial intelligence defeated a Go world champion, Lee Sedol, a feat that was previously believed to be at least a decade away.\n","This event is already a milestone in AI research.\n","\n","A fundamental ingredient of AlphaGo was a neural network architecture that at that time was already revolutionizing the field of computer vision; Convolutional Neural Networks.\n","\n","The huge complexity of the patterns on a Go board was caught with success using the same general priors proved to be effective in classifying real world images.\n","\n","\n","Today we will explore this architecture and one of its evolution, Residual Convolutional Neural Networks, the one used in AlphaGo.\n","\n","If you want to re-experience the AI conquer of the most ancient of games, a very nice documentary about that match is now [free on YouTube](https://www.youtube.com/watch?v=WXuK6gekU1Y)!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mtiCSSY0Qk0N"},"source":["##CNNs: a fundamental idea at the core of deep learning\n","\n","Convolutional neural networks are a way of exploiting a known prior about our problem. Actually, a whole set of priors at the same time:\n","- Translational equivariance\n","- Compositionality\n","- Locality\n","- Self-similarity\n","\n","> **EXERCISE**: Consider the problem of recognizing objects in images. Looking at the following example, discuss how all these priors apply.\n",">\n",">![polyfit matrix notation](https://drive.google.com/uc?export=view&id=1SLeKcN9EAYhy98aaPjxfS7uRyypSSwKS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJnY23_YRLmJ","cellView":"form"},"outputs":[],"source":["# @title Solution 👀\n","# \"\"\"\n","# - Translational equivariance: A book upper left is still a book down right.\n","# - Compositionality: The table is composed by legs, recognizing the legs helps in recognizing the table.\n","# - Locality: To recognize an eye you need only pixels locally concentrated in a spot on the image.\n","# - Self-similarity: You can recognize the two eyes using a single eye-recognizer.\n","# \"\"\""]},{"cell_type":"markdown","metadata":{"id":"NSjsmMzURJm1"},"source":[">Think about the convolution operation as a **fundamental idea at the core of deep learning** more than a nice trick to exploit known but not essential priors.\n","\n","These priors are so general, and training a deep MLP is so hard, that a large part of deep learning applications would be not possible without CNNs.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yjyYfdR7QnrN"},"source":["## CNNs in practice\n","\n","Building a CNN today is very easy thanks to modern deep learning programming frameworks. As you have seen in the last notebook, the `torch.nn` package reduces the introduction of a convolution transformation to a single call to a function  (e.g. ```nn.Conv2d```  for 2D data).\n","\n","Nevertheless, CNNs involve many different operations, including non-trivial changes in shape of the input tensor through the layers.\n","\n","Keep in mind the two basic transformations introduced by CNNs:\n","- Convolution\n","\n","![conv](https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif)\n","- Pooling (Max pooling in the example below)\n","\n","![pooling](https://upload.wikimedia.org/wikipedia/commons/e/e9/Max_pooling.png)\n","\n","*Images from Wikipedia*\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sDbVInBiKs0a"},"source":["---\n","\n","**Under the hood: how does a convolution work?**\n","\n","Let's compute by hand a single output value of a convolution operation.\n","\n","Altough simple to explain intuitively, managing and understanding how the *dimensions* involved behave in the computation can be **outstandingly** confusing. So much so, that implementing [average pooling](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html) with padding and stride using only vectorized code is an interview question at big tech companies!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BX6uFr2TK9dt"},"outputs":[],"source":["# Define an arbitrary input tensor, i.e.: [batch, channels, w, h]\n","batch_size, num_channels, width, height = 1, 2, 4, 4\n","a = torch.arange(batch_size * num_channels * width * height).reshape(batch_size, num_channels, width, height).float()\n","a, a.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p89SvOCjLNix"},"outputs":[],"source":["# Define a convolution\n","c = nn.Conv2d(in_channels=2, out_channels=3, kernel_size=2, bias=False)\n","c"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b61JiSP0Lcq-"},"outputs":[],"source":["# Let's look under the hood... somewhere there must be learnable weights...\n","# They have shape [3, 2, 2, 2], i.e.: [out, in, kernel, kernel]\n","c.weight, c.weight.shape"]},{"cell_type":"markdown","source":["The weights you see above are random, so don't look for any particular meaning. What we care about is the **shape** of `c`:\n","- We asked for $2\\times 2$ kernels.\n","- `nn.Conv2d` creates $2$ such kernels: one per input channel.\n","- Convolution will be applied to each channel separately, resulting in $2$ feature maps.\n","- These features maps are [summed together](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).\n","- All the above is done $3$ times: once per output channel.\n","\n","As a result, the output of `nn.Conv2d` is a tensor with shape `[3, 2, 2, 2]` that will be convolved with the input."],"metadata":{"id":"g2i8ziUi1pTM"}},{"cell_type":"markdown","source":["> **EXERCISE:** What's the shape of the _output_ resulting from applying these convolutions?"],"metadata":{"id":"cEe-fZXB7Tms"}},{"cell_type":"markdown","source":["Let's do a bit of unrolling to get a deeper insight into convolution."],"metadata":{"id":"w7sC8gtX7idF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"shPYD5lQLeyV"},"outputs":[],"source":["# We define custom kernel weights for the first out channel\n","# In this way we can easily reproduce the computation\n","my_custom_2dkernel_in_channel1 = torch.tensor([[ .1, .2], [-.4, -.5, ]])\n","my_custom_2dkernel_in_channel2 = torch.tensor([[.4, .3], [-.42, -.45, ]])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cbwPRBb4Lr5h"},"outputs":[],"source":["my_custom_2dkernel_in_channel1, my_custom_2dkernel_in_channel1.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-X8884WUMTgr"},"outputs":[],"source":["my_custom_2dkernel_in_channel2, my_custom_2dkernel_in_channel2.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dl9dAc8LMXDJ"},"outputs":[],"source":["# Modify the weights of the convolution\n","\n","# out-channel 0, in-channel 0\n","c.weight.data[0, 0, ...] = my_custom_2dkernel_in_channel1\n","\n","# out-channel 0, in-channel 1\n","c.weight.data[0, 1, ...] = my_custom_2dkernel_in_channel2\n","\n","# ...accessing directly the .data attribute of a tensor by-passed some safety checks!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Uqafi2mMd7x"},"outputs":[],"source":["# Let's check: the tirst out-channel is correctly set\n","c.weight, c.weight.shape"]},{"cell_type":"markdown","source":["As we mentioned, we have a $2 \\times 2$ kernel for each input channel, in this case two, and we have as many kernel pairs as we have output channels. For simplicity, we will only consider one output channel in our unrolling."],"metadata":{"id":"VOVMd17p410R"}},{"cell_type":"markdown","metadata":{"id":"4IcR2UxQWnn-"},"source":["> **EXERCISE**\n",">\n","> Stop one second. Try to apply the convolution yourself!\n",">\n","> Here is an input image with shape `torch.Size([1, 2, 4, 4])`, i.e. `[batch, channels, w, h]`:\n",">\n","> ```python\n","> a = tensor([[[[ 0.,  1.,  2.,  3.],\n",">               [ 4.,  5.,  6.,  7.],\n",">               [ 8.,  9., 10., 11.],\n",">               [12., 13., 14., 15.]],\n",">     \n",">              [[16., 17., 18., 19.],\n",">               [20., 21., 22., 23.],\n",">               [24., 25., 26., 27.],\n",">               [28., 29., 30., 31.]]]])\n","> ```\n",">\n","> This is the kernel tensor from before, having shape `torch.Size([3, 2, 2, 2])`, i.e. `[out_channels, in_channels, kernel_size, kernel_size]`:\n",">\n","> ```python\n","> c.weight = tensor([[[[ 0.1000,  0.2000],\n",">                      [-0.4000, -0.5000]],\n",">            \n",">                     [[ 0.4000,  0.3000],\n",">                      [-0.4200, -0.4500]]],\n",">            \n",">            \n",">                    [[[-0.3517,  0.2366],\n",">                      [ 0.2679,  0.1289]],\n",">            \n",">                     [[-0.2465, -0.3489],\n",">                      [-0.2871,  0.2636]]],\n",">            \n",">            \n",">                    [[[ 0.1697,  0.2975],\n",">                      [ 0.1852,  0.0895]],\n",">            \n",">                     [[-0.0035, -0.2689],\n",">                      [-0.3029, -0.3307]]]], requires_grad=True)\n","> ```\n",">\n","> What value will be in `output[0, 0, 0, 0]`, i.e. the first element of the output tensor after the convolution?\n",">\n","> **Do not write code to do this calculation for you**: simply use a calculator and check if you get $-10.25$. If you got this result, you understood this part!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GM64GGyGRmgv"},"outputs":[],"source":["# Here's the complete result when we apply this convolution\n","o = c(a)\n","o, o.shape"]},{"cell_type":"markdown","source":["Is shape `[1, 3, 3, 3]` your answer to the previous exercise? If yes, you're on a good path!"],"metadata":{"id":"EnmVVrBR8BwR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Cek6YsiUVMr"},"outputs":[],"source":["# Let's compute the first value of the first out channel manually, i.e. this one:\n","o[0, 0, 0, 0]  # [batch, channels, w, h]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"66lyzn5mW6US"},"outputs":[],"source":["# Take the first window of the same size of the kernel in the first in_channel of the input\n","f1 = a[0, 0, :2, :2]\n","f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VXkLuX7XXapU"},"outputs":[],"source":["# And the second input channel\n","f2 = a[0, 1, :2, :2]\n","f2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lvpbc_7NXhhs"},"outputs":[],"source":["# Perform a point-wise multiplication along the (input) feature dimension, between the input and the kernel.\n","# In this case this can be done manually in this way:\n","\n","f = f1 * my_custom_2dkernel_in_channel1 + f2 * my_custom_2dkernel_in_channel2\n","f"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EXtIrXlQXkSA"},"outputs":[],"source":["# Sum up the result\n","s = f.sum()\n","s"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xCeR9huCXx8F"},"outputs":[],"source":["# i.e. the first element in the first out_channel of our output tensor\n","o"]},{"cell_type":"markdown","metadata":{"id":"ZHXMHMgCdDE3"},"source":["> **EXERCISE**\n",">\n","> How many parameters does this layer have? Note that this time there is a _bias_ as well! Check the docs to see how bias is defined for 2d convolutions.\n",">\n","> ```python\n","> nn.Conv2d(in_channels=5, out_channels=10, kernel_size=2, bias=True)\n","> ```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJvrkCqmdpMZ","cellView":"form"},"outputs":[],"source":["#@title Solution 👀\n","\n","c =  nn.Conv2d(in_channels=5, out_channels=10, kernel_size=2, bias=True)\n","sum(x.numel() for x in c.parameters() if x.requires_grad)\n","\n","# Can you explain the result?\n","# How big is the bias?"]},{"cell_type":"markdown","source":["### Break the symmetry!\n","\n","A small interlude before we implement our first CNN. Let's address two questions.\n","\n","**_Why would I want more output channels than input channels?_**\n","\n","Having multiple output channels (i.e. _feature maps_) allows the network to simultaneously learn a variety of features from the same piece of input data. For instance, one channel might become specialized in detecting horizontal edges, while another might focus on vertical edges, and a third might detect areas of high contrast.\n","\n","**_If I initialize the kernel weights equally for all the output channels, will I get the same feature maps?_**\n","\n","If you initialize the convolutional filter weights equally for all output channels, indeed you might find that the filters learn identical features and produce the same output. This is why **random initialization** is critical and universally adopted — it ensures that each filter starts from a slightly different state, allowing them to explore different paths and learn to capture various features of the input data."],"metadata":{"id":"oveSkHBWCc7a"}},{"cell_type":"markdown","metadata":{"id":"ADbJXwrYGpWx"},"source":["### Load the dataset CIFAR-10\n","\n","Let's ramp things up a bit and consider something harder than MNIST!\n","\n","The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. These are split into 50,000 training and 10,000 test images.\n","\n","We will use some PyTorch utilities to download, shuffle, normalize the data and arrange it in batches.\n"]},{"cell_type":"code","source":["# Just a function to count the number of parameters\n","def count_parameters(model: torch.nn.Module) -> int:\n","  \"\"\" Counts the number of trainable parameters of a module\n","\n","  :param model: model that contains the parameters to count\n","  :returns: the number of parameters in the model\n","  \"\"\"\n","  return sum(p.numel() for p in model.parameters() if p.requires_grad)"],"metadata":{"id":"l4ZmV7nVExY5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the device to use: use the gpu runtime if possible!\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device: {device}')"],"metadata":{"id":"vXvRduxpEy7R"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xsOaBrQ9HCVZ"},"outputs":[],"source":["# quick and dirty just for this notebook\n","image_transforms = transforms.Compose(\n","    [\n","        transforms.Grayscale(num_output_channels=1),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=(0.5,), std=(0.5,)),\n","    ]\n",")\n","\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.CIFAR10(\n","        \"../data\",\n","        train=True,\n","        download=True,\n","        transform=image_transforms\n","    ),\n","    batch_size=64,\n","    shuffle=True,\n",")\n","\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.CIFAR10(\n","        \"../data\",\n","        train=False,\n","        transform=image_transforms\n","    ),\n","    batch_size=1000,\n","    shuffle=True,\n",")\n","\n","# Retrieve the image size and the number of color channels\n","x, yy = next(iter(train_loader))\n","\n","n_channels = x.shape[1]\n","input_size_w = x.shape[2]\n","input_size_h = x.shape[3]\n","input_size = input_size_w * input_size_h\n","\n","# Specify the number of classes in CIFAR10\n","output_size = yy.max().item() + 1  # there are 10 classes\n","output_classes = ('plane', 'car', 'bird', 'cat', 'deer',\n","                  'dog', 'frog', 'horse', 'ship', 'truck')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PjIHkUug67Ej"},"outputs":[],"source":["# Remember that even if we defined a data loader, we can still directly access\n","# the dataset.\n","train_loader.dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"sGrqPutS6fy2"},"outputs":[],"source":["# @title Dataset examples { run: \"auto\" }\n","\n","import plotly.express as px\n","\n","training_index = 11386  #@param {type:\"slider\", min:0, max:50000, step:1}\n","image, label = train_loader.dataset[training_index]\n","\n","antbee_example = np.array(image[0])\n","\n","fig = px.imshow(antbee_example,\n","                title=output_classes[label],\n","                color_continuous_scale='gray',\n","                color_continuous_midpoint=0)\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"IX6jEEVTHGiZ"},"source":["### Define the model classes\n","\n","We're going to compare the performance of a classic MLP model with a model that uses convolutions.\n","\n","The models are defined as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dw8vFbaQHOwL"},"outputs":[],"source":["class FC2Layer(nn.Module):\n","    def __init__(\n","        self, input_size: int, input_channels: int, n_hidden: int, output_size: int\n","    ) -> None:\n","        \"\"\"\n","        Simple MLP model\n","\n","        :param input_size: number of pixels in the image\n","        :param input_channels: number of color channels in the image\n","        :param n_hidden: size of the hidden dimension to use\n","        :param output_size: expected size of the output (e.g. number of classes if you are in a classification task)\n","        \"\"\"\n","        super().__init__()\n","        self.network = nn.Sequential(\n","            nn.Linear(input_size * input_channels, n_hidden),\n","            nn.ReLU(),\n","            nn.Linear(n_hidden, n_hidden),\n","            nn.ReLU(),\n","            nn.Linear(n_hidden, output_size),\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        :param x: batch of images with size [batch, 1, w, h]\n","\n","        :returns: predictions with size [batch, output_size]\n","        \"\"\"\n","        x = x.view(x.shape[0], -1)\n","        o = self.network(x)\n","        return o\n","\n","\n","class CNN(nn.Module):\n","    def __init__(\n","        self, input_size: int, input_channels: int, n_feature: int, output_size: int\n","    ) -> None:\n","        \"\"\"\n","        Simple model that uses 3x3 convolutions\n","\n","        :param input_size: number of pixels in the image\n","        :param input_channels: number of color channels in the image\n","        :param n_feature: size of the hidden dimensions to use (i.e. output channels for the conv layers)\n","        :param output_size: expected size of the output\n","        \"\"\"\n","        super().__init__()\n","        self.n_feature = n_feature\n","        self.conv1 = nn.Conv2d(\n","            in_channels=input_channels, out_channels=n_feature, kernel_size=3\n","        )\n","        self.conv2 = nn.Conv2d(n_feature, n_feature, kernel_size=3)\n","        self.conv3 = nn.Conv2d(n_feature, n_feature, kernel_size=3)\n","\n","        self.fc1 = nn.Linear(n_feature * 5 * 5, output_size)  # how did we choose those 5? keep reading!\n","        self.fc2 = nn.Linear(output_size, output_size)\n","\n","    def forward(self,\n","                x: torch.Tensor,\n","                return_conv1: bool = False,\n","                return_conv2: bool = False,\n","                return_conv3: bool = False\n","        ) -> torch.Tensor:\n","        \"\"\"\n","        :param x: batch of images with size [batch, 1, w, h]\n","        :param return_conv1: if True return the feature maps of the first convolution\n","        :param return_conv2: if True return the feature maps of the second convolution\n","        :param return_conv3: if True return the feature maps of the third convolution\n","\n","        :returns: predictions with size [batch, output_size]\n","        \"\"\"\n","        x = self.conv1(x)\n","        if return_conv1:\n","            return x\n","\n","        x = F.relu(x)\n","        x = F.max_pool2d(x, kernel_size=2)\n","\n","        x = self.conv2(x)\n","        if return_conv2:\n","            return x\n","\n","        x = F.relu(x)\n","\n","        # Not so easy to keep track of shapes... right?\n","        # A useful trick while debugging is to feed the model a fixed sample batch,\n","        # and print the shape at each step, just to make sure that they match your expectations.\n","\n","        # print(x.shape)\n","\n","        x = self.conv3(x)\n","        if return_conv3:\n","            return x\n","\n","        x = F.relu(x)\n","        x = F.max_pool2d(x, kernel_size=2)\n","\n","        x = x.view(x.shape[0], -1)\n","        x = self.fc1(x)\n","\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","\n","        return x\n","\n","# A fixed sample batch\n","# x, _ = next(iter(train_loader))\n","# model = CNN(input_size, n_channels, 9, 10)\n","# _ = model(x)"]},{"cell_type":"markdown","metadata":{"id":"iGDtNQ9VHQTG"},"source":["### Training utility functions\n","\n","Let's define some utility functions, we'll use them to simplify the training of different models on different datasets.\n","\n","We will need:\n","- A function to permute the pixels of every image in a batch given a fixed permutation (this will be useful later for an experiment)\n","- One function to train the model and evaluate it\n","\n","---\n","\n","*Side note*: you should not memorize the code, just as you do not memorize formulas. You should be able to write it on your own after you understand what it does. Learn what it is possible to do, what should be possible to do and what you need, then... just search for the stuff you need in the docs, google, stackoverflow, or (new in 2024!) a LLM like ChatGPT.\n","\n","If you are troubled when solving algorithmic problems, you can have some fun with the computer science version of puzzle magazines: [LeetCode](https://leetcode.com/problemset/algorithms/).\n","\n","![](https://imgs.xkcd.com/comics/wisdom_of_the_ancients.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bf_B4Mk0HV5T"},"outputs":[],"source":["# You can skip this cell and read only the function docstring\n","\n","import torch\n","import torch.nn.functional as F\n","from typing import Optional, Callable, Dict\n","from tqdm.notebook import tqdm, trange\n","\n","\n","def permute_pixels(images: torch.Tensor, perm: Optional[torch.Tensor]) -> torch.Tensor:\n","    \"\"\" Permutes the pixel in each image in the batch\n","\n","    :param images: a batch of images with shape [batch, channels, w, h]\n","    :param perm: a permutation with shape [w * h]\n","\n","    :returns: the batch of images permuted according to perm\n","    \"\"\"\n","    if perm is None:\n","        return images\n","\n","    batch_size = images.shape[0]\n","    n_channels = images.shape[1]\n","    w = images.shape[2]\n","    h = images.shape[3]\n","    images = images.view(batch_size, n_channels, -1)\n","    images = images[..., perm]\n","    images = images.view(batch_size, n_channels, w, h)\n","    return images\n","\n","\n","def make_averager() -> Callable[[Optional[float]], float]:\n","    \"\"\" Returns a function that maintains a running average\n","\n","    :returns: running average function\n","    \"\"\"\n","    count = 0\n","    total = 0\n","\n","    def averager(new_value: Optional[float]) -> float:\n","        \"\"\" Running averager\n","\n","        :param new_value: number to add to the running average,\n","                          if None returns the current average\n","        :returns: the current average\n","        \"\"\"\n","        nonlocal count, total\n","        if new_value is None:\n","            return total / count if count else float(\"nan\")\n","        count += 1\n","        total += new_value\n","        return total / count\n","\n","    return averager\n","\n","def test_model(\n","    test_dl: torch.utils.data.DataLoader,\n","    model: torch.nn.Module,\n","    perm: Optional[torch.Tensor] = None,\n","    device: str = \"cuda\",\n",") -> Dict[str, Union[float, Callable[[Optional[float]], float]]]:\n","    \"\"\"Compute model accuracy on the test set\n","\n","    :param test_dl: the test dataloader\n","    :param model: the model to train\n","    :param perm: if not None, permute the pixel in each image according to perm\n","\n","    :returns: computed accuracy\n","    \"\"\"\n","    model.eval()\n","    test_loss_averager = make_averager()  # mantain a running average of the loss\n","    correct = 0\n","    for data, target in test_dl:\n","        # send to device\n","        data, target = data.to(device), target.to(device)\n","\n","        if perm is not None:\n","            data = permute_pixels(data, perm)\n","\n","        output = model(data)\n","\n","        test_loss_averager(F.cross_entropy(output, target))\n","\n","        # get the index of the max probability\n","        pred = output.max(1, keepdim=True)[1]\n","        correct += pred.eq(target.view_as(pred)).cpu().sum().item()\n","\n","    return {\n","        \"accuracy\": 100.0 * correct / len(test_dl.dataset),\n","        \"loss_averager\": test_loss_averager,\n","        \"correct\": correct,\n","    }\n","\n","def fit(\n","    epochs: int,\n","    train_dl: torch.utils.data.DataLoader,\n","    test_dl: torch.utils.data.DataLoader,\n","    model: torch.nn.Module,\n","    opt: torch.optim.Optimizer,\n","    tag: str,\n","    perm: Optional[torch.Tensor] = None,\n","    device: str = \"cuda\",\n",") -> float:\n","    \"\"\"Train the model and computes metrics on the test_loader at each epoch\n","\n","    :param epochs: number of epochs\n","    :param train_dl: the train dataloader\n","    :param test_dl: the test dataloader\n","    :param model: the model to train\n","    :param opt: the optimizer to use to train the model\n","    :param tag: description of the current model\n","    :param perm: if not None, permute the pixel in each image according to perm\n","\n","    :returns: accucary on the test set in the last epoch\n","    \"\"\"\n","    for epoch in trange(epochs, desc=\"train epoch\"):\n","        model.train()\n","        train_loss_averager = make_averager()  # mantain a running average of the loss\n","\n","        # TRAIN\n","        tqdm_iterator = tqdm(\n","            enumerate(train_dl),\n","            total=len(train_dl),\n","            desc=f\"batch [loss: None]\",\n","            leave=False,\n","        )\n","        for batch_idx, (data, target) in tqdm_iterator:\n","            # send to device\n","            data, target = data.to(device), target.to(device)\n","\n","            if perm is not None:\n","                data = permute_pixels(data, perm)\n","\n","            output = model(data)\n","            loss = F.cross_entropy(output, target)\n","            loss.backward()\n","            opt.step()\n","            opt.zero_grad()\n","\n","            train_loss_averager(loss.item())\n","\n","            tqdm_iterator.set_description(\n","                f\"train batch [avg loss: {train_loss_averager(None):.3f}]\"\n","            )\n","            tqdm_iterator.refresh()\n","\n","        # TEST\n","        test_out = test_model(test_dl, model, perm, device)\n","\n","        print(\n","            f\"Epoch: {epoch}\\n\"\n","            f\"Train set: Average loss: {train_loss_averager(None):.4f}\\n\"\n","            f\"Test set: Average loss: {test_out['loss_averager'](None):.4f}, \"\n","            f\"Accuracy: {test_out['correct']}/{len(test_dl.dataset)} \"\n","            f\"({test_out['accuracy']:.0f}%)\\n\"\n","        )\n","    models_accuracy[tag] = test_out['accuracy']\n","    return test_out['accuracy']\n","\n","\n","\n","def get_model_optimizer(model: torch.nn.Module) -> torch.optim.Optimizer:\n","    \"\"\"\n","    Encapsulate the creation of the model's optimizer, to ensure that we use the\n","    same optimizer everywhere\n","\n","    :param model: the model that contains the parameters to optimize\n","\n","    :returns: the model's optimizer\n","    \"\"\"\n","    return optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n","    # return optim.SGD(model.parameters(), lr=0.01, momentum=0.1, weight_decay=1e-5)\n"]},{"cell_type":"markdown","metadata":{"id":"YRlcq0a-umG9"},"source":["Training hyperparameters:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0P5wyMj0f1-0"},"outputs":[],"source":["# Define the number of the epochs\n","epochs = 4\n","\n","# Number of hidden units for the MLP (hidden units = neurons = rows of the weight matrix)\n","n_hidden = 9\n","\n","# Number of the feature maps in the CNN (feature maps = output channels resulting from convolutions)\n","n_features = 6\n","\n","# Define a global dictionary to store the performance of the different models\n","models_accuracy = {}"]},{"cell_type":"markdown","metadata":{"id":"7uwKwXBOXl6I"},"source":["#### CIFAR-10\n","\n","We are now ready to train our MLP and CNN on the training set of CIFAR-10,\n","and evaluate the trained models on the test set."]},{"cell_type":"markdown","metadata":{"id":"XPjf_To0HjhU"},"source":["##### Small fully-connected network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HqsZ7Q_-HnjN"},"outputs":[],"source":["model_fnn = FC2Layer(input_size, n_channels, n_hidden, output_size)\n","model_fnn.to(device)\n","optimizer = get_model_optimizer(model_fnn)\n","\n","print(f'Number of parameters: {count_parameters(model_fnn)}')\n","\n","fit(epochs=epochs,\n","    train_dl=train_loader,\n","    test_dl=test_loader,\n","    model=model_fnn,\n","    opt=optimizer,\n","    tag='fnn',\n","    device=device)"]},{"cell_type":"markdown","source":["You have seen right: we get a measly **33% accuracy** (at best) on CIFAR-10 if we train a MLP with **~9400 parameters**. Much worse than our free-lunch 90% score on MNIST, and much harder to improve by changing the hyperparameters alone!\n","\n","Try to play with the size of the hidden layer, e.g. by setting `n_hidden = 20`. How accurate can you get?"],"metadata":{"id":"lffGJGouZxZ1"}},{"cell_type":"markdown","metadata":{"id":"g7izp5VNHo_k"},"source":["##### Even smaller ConvNet\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vXwIAMl8HuZQ"},"outputs":[],"source":["model_cnn = CNN(input_size, n_channels, n_features, output_size)\n","model_cnn.to(device)\n","optimizer = get_model_optimizer(model_cnn)\n","\n","print(f'Number of parameters: {count_parameters(model_cnn)}')\n","\n","fit(epochs=epochs,\n","    train_dl=train_loader,\n","    test_dl=test_loader,\n","    model=model_cnn,\n","    opt=optimizer,\n","    tag='cnn',\n","    device=device)"]},{"cell_type":"markdown","metadata":{"id":"pK6VmMQsHvuq"},"source":["The ConvNet performs better with **~2300 (thus fewer!) parameters**, thanks to weight sharing and its use of priors! Look at that accuracy at a fraction of the cost. If you try to play with the MLP size, you'll see that you need more than 50k parameters to reach a similar score!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzngYxprwCZ4"},"outputs":[],"source":["print(f'FNN number of parameters: {count_parameters(model_fnn)}')\n","print(f'CNN number of parameters: {count_parameters(model_cnn)}')"]},{"cell_type":"markdown","source":["Still, this CIFAR-10 dataset looks way harder than our previous toy problems. Finally, a good challenge for us 💪"],"metadata":{"id":"eC8OcTKLduiF"}},{"cell_type":"markdown","source":["> **EXERCISE:** Can you reach 50% test accuracy with our simple CNN? _(this might take some time, perhaps better do this when you are done with the rest of the notebook)_"],"metadata":{"id":"EzC6h-8UeL7A"}},{"cell_type":"markdown","metadata":{"id":"2OmZDiCtH8wo"},"source":["####  Scrambled CIFAR-10\n","\n","What happens if our assumptions are no longer true?\n","Let's try to randomly permute the pixels in each image, thus **breaking our assumptions of real-world images**!\n","\n","We will use a fixed permutation to shuffle the pixels:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HpnSOEMvRxkO"},"outputs":[],"source":["# Define a permutation of the pixels\n","perm = torch.randperm(input_size)\n","perm"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"hDRLtUNy8uyr"},"outputs":[],"source":["# @title Scrambled dataset examples { run: \"auto\" }\n","\n","import plotly.express as px\n","\n","training_index = 9619  #@param {type:\"slider\", min:0, max:50000, step:1}\n","\n","image, label = train_loader.dataset[training_index]\n","image_perm = permute_pixels(image[None, :], perm).squeeze(dim=0)\n","\n","# To plot a colored image, just use: torch.einsum('cwh -> whc', image)\n","fig = px.imshow(image[0],\n","                title=output_classes[label],\n","                color_continuous_scale='gray',\n","                color_continuous_midpoint=0\n","                )\n","fig.show()\n","\n","fig = px.imshow(image_perm[0],\n","                title=f'Scrambled {output_classes[label]}',\n","                color_continuous_scale='gray',\n","                color_continuous_midpoint=0)\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"f2JrIA7VIJY9"},"source":["##### Small fully-connected network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ej9eAYmGIX0w"},"outputs":[],"source":["model_scrambled_fnn = FC2Layer(input_size, n_channels, n_hidden, output_size)\n","model_scrambled_fnn.to(device)\n","optimizer = get_model_optimizer(model_scrambled_fnn)\n","\n","print(f'Number of parameters: {count_parameters(model_scrambled_fnn)}')\n","\n","fit(epochs=epochs,\n","    train_dl=train_loader,\n","    test_dl=test_loader,\n","    model=model_scrambled_fnn,\n","    opt=optimizer,\n","    tag='scrambled_fnn',\n","    perm=perm,   # <----- permute the images\n","    device=device)"]},{"cell_type":"markdown","metadata":{"id":"3_vkzACuIC4H"},"source":["##### ConvNet with less parameters\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y-wZWiWfIHvq"},"outputs":[],"source":["model_scrambled_cnn = CNN(input_size, n_channels, n_features, output_size)\n","model_scrambled_cnn.to(device)\n","optimizer = get_model_optimizer(model_scrambled_cnn)\n","\n","print(f'Number of parameters: {count_parameters(model_scrambled_cnn)}')\n","\n","fit(epochs=epochs,\n","    train_dl=train_loader,\n","    test_dl=test_loader,\n","    model=model_scrambled_cnn,\n","    opt=optimizer,\n","    tag='scrambled_cnn',\n","    perm=perm,  # permute the images\n","    device=device)"]},{"cell_type":"markdown","metadata":{"id":"G-DqapmTIoBu"},"source":["##### Performance comparison\n","\n","\n","CNN's performance drops when we permute the pixels, but the MLP's performance stays the same! By construction of the convolution operation, **CNNs assume that pixels lie on a grid**, and that patterns are local. If the assumption breaks, so do the CNN's benefits.\n","\n","The MLP does not make any such assumption, hence the pixel ordering does not affect performance.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YlJ6MXpTsTqj"},"outputs":[],"source":["import plotly.express as pe\n","import plotly.graph_objects as go\n","\n","tags = ('fnn', 'cnn', 'scrambled_cnn', 'scrambled_fnn')\n","accuracy_list = [models_accuracy[tag] for tag in tags]\n","\n","fig = go.Figure([go.Bar(x=tags, y=accuracy_list)])\n","fig.update_layout(title='Performance comparison',\n","                  yaxis_title=\"Accuracy [%]\",\n","                  xaxis_title=\"Model type\",\n","                  width=700,\n","                  height=350)\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"2Hh6cbydDFxI"},"source":["*Tutorial on convolutions adapted from this [tutorial](https://github.com/Atcold/pytorch-Deep-Learning/blob/master/06-convnet.ipynb)*"]},{"cell_type":"markdown","metadata":{"id":"i1JIPOKxR0u9"},"source":["### **EXERCISE**\n",">\n","> What do you think will happen if you apply a different, independent permutation to each image?\n",">\n","> If you are not sure, try to do it and check again the performance!\n",">\n","> *Hint:* to do this, it is enough to change the `fit` function and re-run the previous cells.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qi1YhlxfoBuz"},"source":["### **EXERCISE**\n","\n","> The state-of-the-art (SOTA) accuracy for the classification of _color_ CIFAR-10 (remember that we transformed our images to grayscale) is $96.53$%. You can check out the ranking [here](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130) up to 2015.\n",">\n","> Our CNN barely reaches $40$% on grayscale CIFAR-10...\n",">\n","> For this exercise, adapt our code to deal with full-color CIFAR-10. How much gain in accuracy do you get using the RGB color information?\n",">\n","> Some examples of other things you can tweak:\n",">\n","> - Model architecture\n","> - Data normalization (e.g. normalize using the train set mean and std)\n","> - Optimizer and its parameters\n","> - Batch size\n",">\n","> How high can you go in the ranking?"]},{"cell_type":"markdown","metadata":{"id":"SBrsCHpiDUbZ"},"source":["### Playground: CNN visualization\n","\n","It is possible to visualize the internals of the CNN in two possible ways:\n","\n","- You can visualize the learned kernels\n","- You can visualize how the kernels modify the input, i.e. their feature maps\n","\n","In the following cell you can explore the feature maps, which are easier to interpret for us if compared to the kernels themselves!"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"I1lYFu_8Di64"},"outputs":[],"source":["#@title Feature maps visualization { run: \"auto\" }\n","\n","import plotly.express as px\n","\n","test_index = 941  #@param {type:\"slider\", min:0, max:10000, step:1}\n","scramble_image = False #@param {type:\"boolean\"}\n","\n","\n","image, label = test_loader.dataset[test_index]\n","\n","if scramble_image:\n","    image = permute_pixels(image[None, :], perm).squeeze(dim=0)\n","\n","imabe_cuda = image.to(device)\n","\n","\n","\n","show_input_image = True #@param {type:\"boolean\"}\n","if show_input_image:\n","    # To plot a colored image, just use: torch.einsum('cwh -> whc', image)\n","    fig = px.imshow(image[0],\n","                    title=f'Input image [{output_classes[label]}]',\n","                    color_continuous_scale='gray',\n","                    color_continuous_midpoint=0\n","                    )\n","    fig.show()\n","\n","\n","trained_on = {\n","    'scrambled images': model_scrambled_cnn,\n","    'normal images': model_cnn\n","}\n","select_model_trained_on = \"normal images\" #@param [\"scrambled images\", \"normal images\"]\n","model = trained_on[select_model_trained_on]\n","\n","show_conv1_feature_maps = True #@param {type:\"boolean\"}\n","select_conv1_feature_map = 4  #@param {type:\"slider\", min:0, max:5, step:1}\n","\n","if show_conv1_feature_maps:\n","    conv1_features = model(imabe_cuda[None, ...], return_conv1=True).cpu().detach()\n","    conv1_features = conv1_features[0, select_conv1_feature_map, ...]\n","    fig = px.imshow(conv1_features,\n","                    title=f'{select_conv1_feature_map}-th feature maps of conv1 [{output_classes[label]}]',\n","                    color_continuous_scale='gray',\n","                    color_continuous_midpoint=0)\n","    fig.show()\n","\n","show_conv2_feature_maps = True #@param {type:\"boolean\"}\n","select_conv2_feature_map = 3  #@param {type:\"slider\", min:0, max:5, step:1}\n","\n","if show_conv2_feature_maps:\n","    conv2_features = model(imabe_cuda[None, ...], return_conv2=True).cpu().detach()\n","    conv2_features = conv2_features[0, select_conv2_feature_map, ...]\n","    fig = px.imshow(conv2_features,\n","                    title=f'{select_conv2_feature_map}-th feature maps of conv2 [{output_classes[label]}]',\n","                    color_continuous_scale='gray',\n","                    color_continuous_midpoint=0)\n","    fig.show()\n","\n","\n","show_conv3_feature_maps = False #@param {type:\"boolean\"}\n","select_conv3_feature_map = 2  #@param {type:\"slider\", min:0, max:5, step:1}\n","\n","if show_conv3_feature_maps:\n","    conv3_features = model(imabe_cuda[None, ...], return_conv3=True).cpu().detach()\n","    conv3_features = conv3_features[0, select_conv3_feature_map, ...]\n","    fig = px.imshow(conv3_features,\n","                    title=f'{select_conv3_feature_map}-th feature maps of conv3 [{output_classes[label]}]',\n","                    color_continuous_scale='gray',\n","                    color_continuous_midpoint=0)\n","    fig.show()"]},{"cell_type":"markdown","metadata":{"id":"GKfdypNsLEF0"},"source":["#### Interactive visualization\n","\n","You can find a much nicer interactive visualization to explore completely the architecture of MLPs and CNNs at this [link](https://adamharley.com/nn_vis/cnn/2d.html).\n","\n","You can write your own MNIST number and see how each hidden unit is computed and activated!\n","\n","![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/06/pics/vis.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xD8G2aN7_prq"},"source":["## Residual networks\n","\n","The last part of this tutorial is about residual networks, the first architectures to go really deep in deep learning [[He et al. 2015](https://arxiv.org/abs/1512.03385)].\n","\n","Residual CNNs are the state-of-the-art architecture in many learning problems, from image classification to reinforcement learning with the AlphaGo saga.\n","\n","In the figure below you see a comparison of several architectures on the [Imagenet classification challenge](https://en.wikipedia.org/wiki/ImageNet#ImageNet_Challenge) (1000 classes); the x-axis of the second plot is the no. of operations needed for a single forward pass, while the size of the blobs is proportional to the number of training parameters. *Inception-v4* and all the *ResNet* are Residual CNNS, the number after *ResNet* is the number of layers.\n","\n",">*How does an Inception network see? Check out [this article](https://distill.pub/2020/circuits/early-vision/) published in April 2020*\n","\n","![residual networks](https://drive.google.com/uc?export=view&id=1pZ8EIMriEtFxi_f3atv0OBJpOgS4CBTs)\n","\n","*Image from [An Analysis of Deep Neural Network Models for Practical Applications](https://arxiv.org/abs/1605.07678)*\n","\n","Quite a few layers, wouldn't you say?\n","\n","**_The deeper, the better._** The trend looks clear. And it is reasonable, deeper models are more *expressive*, they can represent very complex functions, learning features at different levels of abstraction.\n","\n","**_Deeper doesn't mean more weights._** For example, VGG-16 has several million more parameters than ResNet-50, but its depth is less than half. The _width_ of each layer, the depth, the size of the convolutional kernels, and other parameters (e.g. batchnorm, as we'll see in the next lectures) is what determines the total size."]},{"cell_type":"markdown","source":["### Vanishing gradients\n","\n","However, the training of very deep neural networks can be extremely slow. Part of the reason is the following: if the gradients of the network operations (including the activation functions) are less than 1, multiplying many of these small numbers together leads to an exponentially smaller gradient as you progress backward from the output layer to the input layer. For this reason, the weights of the first layers are tuned very slowly, since the norm of their gradients is close to 0.\n","\n","This is a natural consequence of the mathematics involved in deep learning -- but it's not desirable nor useful!\n","\n","---\n","\n","**_Adding a shortcut._**\n","\n","Residual networks solve this problem by introducing shortcuts: **additional paths for gradient flow** that do not undergo multiplication by small numbers, effectively addressing the vanishing gradient issue.\n","\n","<img src=\"https://i.stack.imgur.com/msvse.png\" width=\"500\">\n","\n","This way, even if the gradients on the original path diminish, the gradients on the shortcut path help prevent the total gradient from vanishing.\n","\n","Normally you would have several such **residual blocks** throughout your network, so that this mitigation of the vanishing gradient phenomenon happens layer by layer.\n","\n","---\n","\n","**_Residuals are deviations from identity._**\n","\n","Notice that residual blocks learn a **correction rather than a transformation** of the input. The physicists among you may call it an \"additive perturbation\". This is useful, because a residual block can easily learn the identity map, if that's what is needed by the network to solve the task at hand. In other words, with residual connections, layers in deep networks can adapt more flexibly. Some layers might end up performing minimal transformations when the identity function is close to optimal, while others learn significant deviations from the identity where needed. This **layer-wise adaptability** is crucial for dealing with a wide range of input data and tasks.\n","\n"],"metadata":{"id":"hSipIaXUuXow"}},{"cell_type":"markdown","metadata":{"id":"9MUYKCCn92nb"},"source":["### Transfer learning with residual networks\n","\n","Our final effort today is to build a model able to distinguish ants from bees.\n","\n","Let's download the dataset and visualize some cool insects.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8ZtzWMF98_I"},"outputs":[],"source":["!git clone https://github.com/jaddoescad/ants-bees-dataset.git"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mOCY1YyfLZ7j"},"outputs":[],"source":["# @title Dataset examples { run: \"auto\" }\n","\n","import plotly.express as px\n","import numpy as np\n","\n","dataset_path = '/content/ants-bees-dataset/'\n","vanilla_training_dataset = datasets.ImageFolder(dataset_path + 'train')\n","class_names = ['A lovely ant', 'What a cute bee']\n","\n","training_index = 92  #@param {type:\"slider\", min:0, max:243, step:1}\n","antbee_example = np.array(vanilla_training_dataset[training_index][0])\n","\n","fig = px.imshow(antbee_example, title=class_names[vanilla_training_dataset[training_index][1]])\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sdpoZSCi-j8h"},"outputs":[],"source":["print(f'Samples in the training dataset: {len(vanilla_training_dataset)}')"]},{"cell_type":"markdown","metadata":{"id":"Wd59sho1OkUC"},"source":["Since the dataset is very small and there is not enough meat for training, we are going to do some **transfer learning** magic.\n","\n","Still, we will **augment** the training data by using the image transforms implemented in `torchvision`.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WMvzGfJjDVvy"},"outputs":[],"source":["# Data augmentation and normalization for training\n","\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(224),  # this transform does a different crop every time, de facto augmenting the training dataset\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),  # this transform does always the same crop at the center of the image, we do not want to augment the validation dataset\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","image_datasets = {x: datasets.ImageFolder(os.path.join(dataset_path, x), data_transforms[x]) for x in ['train', 'val']}\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=2) for x in ['train', 'val']}"]},{"cell_type":"markdown","source":["**Why those values for the normalization step?**\n","\n","We are going to use a model that was pre-trained on the ImageNet-1K dataset (1000 object classes and 1,281,167 training images). When using a pre-trained model for transfer learning or fine-tuning, it's essential _to normalize new input data using the same mean and standard deviation_ values that were used during the original training, to ensure consistency and performance.\n","\n","By applying this normalization, the input images are transformed in a way that aligns with the expectations of the network architecture.\n","\n","The specific values are taken from the [docs](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html) (scroll down to the end)."],"metadata":{"id":"Ap6SXJC_dKuL"}},{"cell_type":"markdown","metadata":{"id":"IRLarQdxTpw3"},"source":["Let's visualize a transformed example."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rdB6YjSASf2v"},"outputs":[],"source":["transformed_example = image_datasets['train'][42][0].numpy()\n","print('A transformed training sample of shape', transformed_example.shape)\n","fig = px.imshow(np.swapaxes(transformed_example, 0, 2))  # px.imshow wants the color channels as last dimension\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"O05qbPx0TxEW"},"source":["> **EXERCISE**: Why is it so black? Have we zoomed too much on an ant with the random crops?\n",">\n",">*Hint*: No, look at another tranformation.\n",">\n","> Write a function to properly visualize transformed training samples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E2KuG5CZU06H"},"outputs":[],"source":["# ✏️ your code here\n","\n","def visualize_samples(batch_of_samples, title=None):\n","    \"\"\"\n","    Visualization of transformed samples, a standard call:\n","        inputs, classes = next(iter(dataloaders['train']))\n","        visualize_samples(inputs)\n","    Arguments:\n","    batch_of_samples -- a batch from the dataloader; a PyTorch tensor of shape (batch_size, 3, 224, 224)\n","    title -- (optional) a string with the title of the plot, useful to show the labels\n","\n","    Return:\n","    None (A nice plot with )\n","    \"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"iA87SLm4Dd5v"},"outputs":[],"source":["#@title Solution 👀\n","\n","def visualize_samples(inputs, title=None):\n","    \"\"\"\n","    Visualization of transformed samples, a standard call:\n","        inputs, classes = next(iter(dataloaders['train']))\n","        visualize_samples(inputs)\n","    Arguments:\n","    batch_of_samples -- a batch from the dataloader; a PyTorch tensor of shape (batch_size, 3, 224, 224)\n","\n","    Return:\n","    None (A nice plot)\n","    \"\"\"\n","\n","    # Make a grid from batch\n","    inp = torchvision.utils.make_grid(inputs)\n","\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    inp = std * inp + mean\n","    inp = np.clip(inp, 0, 1)  # plotly accepts the color information both in the 0-1 range and in the 0-255 range\n","    fig = px.imshow(inp, title=title)\n","    fig.show()\n","\n","\n","# Get a batch of training data\n","inputs, classes = next(iter(dataloaders['train']))\n","\n","visualize_samples(inputs, title=f'Ground truth: {[class_names[x] for x in classes]}')"]},{"cell_type":"markdown","metadata":{"id":"aEBOMCbMz0rU"},"source":["It's time to build our model, we are going to load a pre-trained _residual_ CNN using `torchvision`. _(if you are curious about how it was trained, [here](https://github.com/pytorch/vision/tree/main/references/classification) are the hyperparameters)_\n","\n","Since we want to ❄️freeze its weigths, we will set their `requires_grad` attribute to `False`.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qvOiYFdDoKR"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","residual_net = torchvision.models.resnet18(pretrained=True)\n","for param in residual_net.parameters():\n","    param.requires_grad = False\n","\n","# replace the last dense layer\n","num_ftrs = residual_net.fc.in_features  # input dimension of the last layer\n","print(f'input dimension of the last layer: {num_ftrs}')\n","residual_net.fc = nn.Linear(num_ftrs, 2)  # parameters of newly constructed modules have requires_grad=True by default\n","\n","residual_net = residual_net.to(device)\n","\n","loss_func = nn.CrossEntropyLoss()\n","\n","# we optimize only the last layer parameters\n","opt = optim.SGD(residual_net.fc.parameters(), lr=0.001, momentum=0.9)"]},{"cell_type":"markdown","source":["How did we know that the last layer is linear and called `fc`? Here's how:"],"metadata":{"id":"Tu8k00IzjPFe"}},{"cell_type":"code","source":["print(residual_net)"],"metadata":{"id":"Kpm7CZfRi6xb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TNlWCV2OHktm"},"source":["We can actually show a better summary:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IdM4NmH6JZEX"},"outputs":[],"source":["from torchsummary import summary\n","summary(residual_net, image_datasets['train'][0][0].shape, batch_size=-1)  # -1 is a placeholder for any batch size"]},{"cell_type":"markdown","metadata":{"id":"ifeHwhlsK2Bk"},"source":["And now let's have a look at the computational graph built by Autograd! Since we are only training the last layer, the computational graph will only include the operations that happen there."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2TiKeQuhLM6Q"},"outputs":[],"source":["!pip install torchviz"]},{"cell_type":"markdown","source":["In blue you see the trainable parameters, while in gray the backward functions."],"metadata":{"id":"Fw2VE8oyhSSf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0LkEWlKLK8dI"},"outputs":[],"source":["from torchviz import make_dot\n","\n","x = torch.zeros(1, 3, 224, 224).to(device) #, dtype=torch.float, requires_grad=False)\n","out = residual_net(x)\n","make_dot(out)"]},{"cell_type":"markdown","source":["> **EXERCISE:** What are those `(2)` weights?"],"metadata":{"id":"NJcjBJmyilMi"}},{"cell_type":"markdown","metadata":{"id":"gZSgIcVCNd3A"},"source":["If we want to illustrate the computational graph of the _entire_ network, we must reactivate the gradients throughout all the layers.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WVgJz2fkMzjN"},"outputs":[],"source":["for param in residual_net.parameters():\n","    param.requires_grad = True\n","x = torch.zeros(1, 3, 224, 224).to(device) #, dtype=torch.float, requires_grad=False)\n","out = residual_net(x)\n","make_dot(out)"]},{"cell_type":"markdown","metadata":{"id":"S2n6HiU4Po89"},"source":["Can you see the skip connections?"]},{"cell_type":"markdown","metadata":{"id":"Ge9lrqeWQ3ZU"},"source":["Finally let's train our model!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qKoEw6Ij22Q"},"outputs":[],"source":["# Before training we have to freeze again all the parameters except for the last layer\n","for param in residual_net.parameters():\n","    param.requires_grad = False\n","for param in residual_net.fc.parameters():\n","    param.requires_grad = True\n","\n","num_epochs = 20\n","\n","for epoch in range(num_epochs):\n","\n","    # Evaluation\n","    residual_net.eval()\n","    with torch.no_grad():\n","        misclassified = {'train': 0, 'val': 0}\n","        for ds in ['train', 'val']:\n","            for inputs, classes in dataloaders[ds]:\n","                inputs = inputs.to(device)\n","                classes = classes.to(device)\n","                outputs = residual_net(inputs)\n","                _, preds = torch.max(outputs, 1)\n","                misclassified[ds] += np.sum(torch.abs(preds - classes).cpu().numpy())\n","\n","    accuracy_tr = 1 - misclassified['train'] / len(image_datasets['train'])\n","    accuracy_val = 1 - misclassified['val'] / len(image_datasets['val'])\n","    print(f'Epoch: {epoch} \\t Training accuracy: {accuracy_tr:.3f} \\tValidation accuracy: {accuracy_val:.3f}')\n","\n","    # Training\n","    residual_net.train()\n","    for tr_inputs, tr_classes in dataloaders['train']:\n","        tr_inputs = tr_inputs.to(device)\n","        tr_classes = tr_classes.to(device)\n","        pred = residual_net(tr_inputs)\n","        loss = loss_func(pred, tr_classes)\n","\n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-PyLvC4xt2Cn"},"outputs":[],"source":["# re-execute to see new examples\n","inputs, classes = next(iter(dataloaders['val']))\n","_, predictions = torch.max(residual_net(inputs.to(device)), 1)\n","visualize_samples(inputs, title=f' ground truth: {[class_names[x] for x in classes]}<br>predictions: {[class_names[x] for x in predictions]}')"]},{"cell_type":"markdown","metadata":{"id":"mjx3ImUXvlZY"},"source":["*Tutorial on transfer learning adapted from this [tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)*\n","\n",">**EXERCISE**: Fine tune this model again, this time using a [learning rate scheduler](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate). What happens if you freeze less layers? Try to unfreeze the last residual block. And if you use [another pretrained model](https://pytorch.org/vision/0.9/models.html) larger than resnet18?"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["UzZFFAD7ujN4"],"toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}