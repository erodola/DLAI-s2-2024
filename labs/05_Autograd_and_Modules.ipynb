{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["5kgFt-wdVnXI","6-ySDZ7VNa2r","KJC-A03DALRp","OQteSeLHGq74","FkefLsEoJS4Y","IqrAmmOeSx_t","juLqy9vAbMeA","EKmBMm97mwim","Q6_8luL9oo8U","5rg0ZSrmcAlB","v6dqtbeo_3BO","2ZH_ZFvsUjHx","apkyGbXEbu9F","riJ-TeznY5Pr","StzZE8g8KNgI","30r6CUdnjVXq","desrsm7KWlw2","htzNwsSH9_7O","fnFpy_nQ8OVc","nglEF3-I9B0Z","Vt6ZCLWl9aIG","zejrsiY79wnT","TL_gZXtj0oWD","Xwkalx3GSpmG","CG-o5SyW-2Gt","_osaceZPDLNq","YfL4-S6YKzjH","VmFJ7hRMMFRL","8s550An7OwDj","pIYrW4yJPKNG","G7O345TQQBDx","EGh9MSe9RgRh","jT_IS5bbUI83","BEyPjw9iWIbS","kaJphSnyXDp6","nMdlDhLCYQzW","INMixcSqaPOs","F7FZjuoT6Z9E","TZqlt4gxb4IK","F-hWBkrccW0a"],"toc_visible":true,"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5kgFt-wdVnXI"},"source":["# Deep Learning & Applied AI\n","\n","We recommend going through the notebook using Google Colaboratory.\n","\n","# Tutorial 5: Autograd and Modules\n","\n","In this tutorial, we will cover:\n","\n","- Autograd, back-propagation\n","- Modules, `torch.nn`\n","\n","Based on original material by Dr. Luca Moschella, Dr. Antonio Norelli and Dr. Marco Fumero.\n","\n","Course:\n","\n","- Website and notebooks will be available at https://github.com/erodola/DLAI-s2-2024/"]},{"cell_type":"markdown","metadata":{"id":"6-ySDZ7VNa2r"},"source":["## Import dependencies (run the following cells)"]},{"cell_type":"code","metadata":{"id":"pRePt-K1_yw9","cellView":"form"},"source":["# @title import dependencies\n","\n","from typing import Mapping, Union, Optional\n","\n","import numpy as np\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import plotly.graph_objects as go\n","\n","from torchvision import datasets, transforms\n","from tqdm.notebook import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2tGN_bJOcfd3","cellView":"form"},"source":["# @title reproducibility stuff\n","\n","import random\n","torch.manual_seed(42)\n","np.random.seed(42)\n","random.seed(0)\n","\n","torch.cuda.manual_seed(0)\n","torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n","torch.backends.cudnn.benchmark = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bAaQr9d586Lv","cellView":"form"},"source":["# @title utility functions\n","\n","from typing import Callable, Union, Sequence\n","import math\n","\n","def peaks(meshgrid: torch.Tensor) -> torch.Tensor:\n","  \"\"\"\n","  \"Peaks\" function that has multiple local minima.\n","\n","  :params meshgrid: tensor of shape [..., 2], the (x, y) coordinates\n","  \"\"\"\n","  meshgrid = torch.as_tensor(meshgrid, dtype=torch.float)\n","  xx = meshgrid[..., 0]\n","  yy = meshgrid[..., 1]\n","  return (0.25 * (3*(1-xx)**2*torch.exp(-xx**2 - (yy+1)**2) -\n","                  10*(xx/5 - xx**3 - yy**5)*torch.exp(-xx**2-yy**2) -\n","                  1/3*torch.exp(-(xx+1)**2 - yy**2)))\n","\n","\n","def rastrigin(meshgrid: torch.Tensor, shift: int = 0) -> torch.Tensor:\n","  \"\"\"\n","  \"Rastrigin\" function with `A = 3`\n","  https://en.wikipedia.org/wiki/Rastrigin_function\n","\n","  :params meshgrid: tensor of shape [..., 2], the (x, y) coordinates\n","  \"\"\"\n","  meshgrid = torch.as_tensor(meshgrid, dtype=torch.float)\n","  xx = meshgrid[..., 0]\n","  yy = meshgrid[..., 1]\n","  A = 3\n","  return A * 2 + (((xx - shift) ** 2 - A * torch.cos(2 * torch.tensor(math.pi, dtype=torch.float, device=xx.device) * xx))\n","                  +\n","                  ((yy - shift) ** 2 - A * torch.cos(2 * torch.tensor(math.pi, dtype=torch.float, device=xx.device) * yy)))\n","\n","\n","def rosenbrock(meshgrid: torch.Tensor) -> torch.Tensor:\n","  \"\"\"\n","  \"Rosenbrock\" function\n","  https://en.wikipedia.org/wiki/Rosenbrock_function\n","\n","  It has a global minimum at $(x , y) = (a, a^2) = (1, 1)$\n","\n","  :params meshgrid: tensor of shape [..., 2], the (x, y) coordinates\n","  \"\"\"\n","  meshgrid = torch.as_tensor(meshgrid, dtype=torch.float)\n","  xx = meshgrid[..., 0]\n","  yy = meshgrid[..., 1]\n","\n","  a = 1\n","  b = 100\n","  return (a - xx) ** 2 + b * (yy - xx**2)**2\n","\n","\n","def simple_fn(meshgrid: torch.Tensor) -> torch.Tensor:\n","  \"\"\"\n","  :params meshgrid: tensor of shape [..., 2], the (x, y) coordinates\n","  \"\"\"\n","  meshgrid = torch.as_tensor(meshgrid, dtype=torch.float)\n","  xx = meshgrid[..., 0]\n","  yy = meshgrid[..., 1]\n","\n","  output = -1/(1 + xx**2 + yy**2)\n","\n","  return output\n","\n","def simple_fn2(meshgrid: torch.Tensor) -> torch.Tensor:\n","  \"\"\"\n","  :params meshgrid: tensor of shape [..., 2], the (x, y) coordinates\n","  \"\"\"\n","  meshgrid = torch.as_tensor(meshgrid, dtype=torch.float)\n","  xx = meshgrid[..., 0]\n","  yy = meshgrid[..., 1]\n","\n","  output = (1 + xx**2 + yy**2) ** (1/2)\n","\n","  return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6VsXnIyGLsgb","cellView":"form"},"source":["# @title utility plot functions\n","import plotly.express as px\n","\n","def plot_landscape(\n","    fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n","    resolution: int = 100,\n","    lim: int = 3,\n","    height: int = 900,\n","    landscape_opacity: float = 1.0,\n","    title: Optional[str] = None,\n","    autoshow: bool = False,\n","    **kwargs\n",") -> go.Figure:\n","    \"\"\" Plot the landscape defined by the function `fn`.\n","\n","  Creates a domain grid $x,y \\in R^2$ with $x \\in [-lim, lim]$ and\n","  $y \\in [-lim, lim]. The number of points in this grid is resolution**2.\n","  \"\"\"\n","    xx = torch.linspace(-lim, lim, resolution)\n","    yy = torch.linspace(-lim, lim, resolution)\n","\n","    yy = yy.repeat(yy.shape[0], 1)\n","    xx = xx.unsqueeze(-1).repeat(1, xx.shape[0])\n","    meshgrid = torch.stack((xx, yy), dim=-1)\n","    zz = fn(meshgrid, **kwargs)\n","\n","    xx = xx.cpu().detach()\n","    yy = yy.cpu().detach()\n","    zz = zz.cpu().detach()\n","\n","    fig = go.Figure(data=[go.Surface(z=zz, x=xx, y=yy, opacity=landscape_opacity,\n","                                     cmid=0,\n","                                     colorscale='Viridis')])\n","    fig.update_traces(\n","        contours_z=dict(\n","            show=True, usecolormap=True, highlightcolor=\"lightgray\", project_z=True\n","        )\n","    )\n","    fig.update_layout(\n","        height=height,\n","    )\n","\n","    if autoshow:\n","      fig.show()\n","    return fig\n","\n","\n","def plot_points_over_landscape(\n","    fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n","    points: (float, float) = None,\n","    resolution: int = 100,\n","    lim: int = 3,\n","    landscape_opacity: float = 1.0,\n","    height: int = 900,\n","    title: Optional[str] = None,\n","    autoshow: bool = False,\n",") -> go.Figure:\n","    \"\"\" Plot a point over the landascape defined by the cunction `fn`\n","\n","    :param fn: an universal function $R^2 -> R$\n","    :param points: tensor of shape [..., 3]\n","    :param title: the title of the plots, if None defaults to  the fn name\n","    :param autoshow: if True, calls fig.show() before returning the figure\n","\n","    :retuns: the figure that contains the plot\n","    \"\"\"\n","    points = torch.as_tensor(points)\n","    fig = plot_landscape(\n","        fn,\n","        resolution=resolution,\n","        lim=lim,\n","        height=height,\n","        landscape_opacity=landscape_opacity,\n","        title=title\n","    )\n","\n","    # Create starting path\n","    x_points = points[..., 0]\n","    y_points = points[..., 1]\n","    z_points = points[..., 2]\n","\n","    for point in points:\n","      fig.add_trace(\n","          go.Scatter3d(\n","              visible=True,\n","              showlegend=False,\n","              mode=\"markers\",\n","              marker=dict(size=6, color=\"darkred\", symbol=\"circle\"),\n","              x=x_points,\n","              y=y_points,\n","              z=z_points,\n","          )\n","      )\n","\n","    if autoshow:\n","        fig.show()\n","\n","    return fig"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KJC-A03DALRp"},"source":["# Intro\n"]},{"cell_type":"markdown","metadata":{"id":"OQteSeLHGq74"},"source":["# Autograd: automatic differentiation\n","\n"]},{"cell_type":"markdown","source":["Let's begin.\n","\n","---\n","\n","We have already seen many PyTorch features, from linear algebra to the useful ``Dataset`` and ``Optimizer`` classes.\n","\n","And yet, with little effort we could have used the Numpy or Scikit-learn libraries instead of Pytorch to accomplish the feats of the last notebooks.\n","\n","Today we will go through the main _raison d'Ãªtre_ of Pytorch; the ``autograd`` package."],"metadata":{"id":"tJtg3BSt_vYt"}},{"cell_type":"markdown","metadata":{"id":"FkefLsEoJS4Y"},"source":["### Differentiable programming (optimization)\n","\n","\n",">Differentiable programming is a programming paradigm in which the programs can be differentiated throughout, usually via **automatic differentiation**. This allows for gradient based optimization of parameters in the program, often via gradient descent. Differentiable programming has found use in a wide variety of areas, particularly scientific computing and artificial intelligence. (*Wikipedia [entry](https://en.wikipedia.org/wiki/Differentiable_programming) for Differentiable programming*)\n","\n","The ``autograd`` package of PyTorch is here to provide this *automatic differentiation* for all operations on Tensors.\n","\n","It follows a *define-by-run* philosophy, which means that the computational graph used for backpropagation is defined as your code is executed. This allows, for instance, executing iterative optimization processes where every single iteration may be different (!).\n","\n","This is in contrast to the (admittedly more intuitive) idea of running automatic differentiation over a *static computational graph* (which is what TensorFlow 1.0 did).\n","\n","Both these approaches are based on the *reverse-mode automatic differentation*.\n","\n","As seen in theory class, it scales to **high dimensional data** and **very complex computational graphs**, differently from the forward approach.\n","\n","In the context of Neural Networks we refer to the reverse-mode automatic differentation as *backpropagation*."]},{"cell_type":"markdown","metadata":{"id":"IqrAmmOeSx_t"},"source":["## Basics\n","\n","Let's start by defining a tensor $x$ that may appear in some computation like $f(x) = x^2 + x^3$.\n","\n","Suppose we want to calculate its derivative at the point $x=42$:\n","$$\\frac{\\partial f}{\\partial x}\\Bigr\\rvert_{x=42}$$\n","\n","PyTorch does it through the *reverse mode automatic differentiation*, composed of a forward and backward pass, as seen in lecture.\n","\n","Think of the forward pass this way: it performs some computation, and **the execution of each instruction contributes to the construction of the computational graph**."]},{"cell_type":"code","metadata":{"id":"A2WafGqiQZSA"},"source":["x = torch.tensor(42., requires_grad=True)  # we'll compute the gradient w.r.t. this variable!\n","x2 = x ** 2\n","x3 = x ** 3\n","f = x2 + x3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DrM2IBJBU9BQ"},"source":["Now the backward pass, where we'll appreciate the *automatic* part of the differentation: you just need to call the `backward()` method from your output $f$.  "]},{"cell_type":"code","metadata":{"id":"yPzpQ5yaRlRm"},"source":["f.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F-tFl09aaEgE"},"source":["Now you have $\\frac{\\partial f}{\\partial x}\\Bigr\\rvert_{x=42}$ in the `grad` of $x$"]},{"cell_type":"code","metadata":{"id":"3TYHvfmESIRG"},"source":["x.grad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9WJgwUuySJtC"},"source":["2 * 42 + 3 * 42**2  # Yep, it's correct."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"juLqy9vAbMeA"},"source":["Now you know the basics. And this is enough for training very standard models on PyTorch.\n","\n","Nevertheless, the design principles behind the PyTorch `autograd` package are not always as straightforward. For instance, what do you think will happen executing `backward()` a second time?\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"IjrsmwFblEVe"},"source":["# f.backward()  # try!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"222khA6jlBXI"},"source":["\n","To fully understand the world of the `Autograd` package we must go deeply down the rabbit hole.\n","\n","You do not need to get at the first pass everything we are going to mention from now on. There are explanations of advanced concepts and some PyTorch internals, which are usually not needed but can be useful (e.g. in debugging or complex implementations).\n","\n","Feel free to refer back to this notebook when needed!"]},{"cell_type":"markdown","metadata":{"id":"EKmBMm97mwim"},"source":["### `Autograd` aggressive buffer freeing"]},{"cell_type":"markdown","metadata":{"id":"Q6_8luL9oo8U"},"source":["#### The second backward\n","So, what was the problem with the second backward?\n","\n","When we computed the first `backward()`, the intermediate variables needed for the computation of $f$, as well as its gradient, were freed to save memory. So PyTorch does not have the necessary information to do backward from $f$ a second time.\n","\n","`Autograd` has an aggressive buffer freeing policy to be very memory efficient!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ytUCW4Kzpw-V"},"source":["If you want to prevent this, you can use `.backward(retain_graph=True)`.\n","\n","Let's redo from scratch the previous computation:"]},{"cell_type":"code","metadata":{"id":"ER5iDz7wj-hf"},"source":["x = torch.tensor(42., requires_grad=True)\n","x2 = x ** 2\n","x3 = x ** 3\n","f = x2 + x3\n","f.backward(retain_graph=True)\n","f.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jeS9KlJkzriT"},"source":["So we did backward two times. Let's check again the gradient of $x$:"]},{"cell_type":"code","metadata":{"id":"Oxy1Zgm6-5ne"},"source":["x.grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uv-d0Akf-7T2"},"source":["It's doubled!\n","\n","The reason is that `Autograd` keeps accumulating into the `grad` attribute. This means that multiple `backward()` calls **will sum up previously computed gradients** if they are not explicitly zeroed out."]},{"cell_type":"markdown","metadata":{"id":"5rg0ZSrmcAlB"},"source":["#### Intermediate gradients are not kept by default\n","Intermediate gradients are other victims of PyTorch's aggressive buffer freeing policy.\n","\n","We do not have access to the gradient with respect to $x_2$, even if we actually computed it to calculate the one with respect to $x$."]},{"cell_type":"code","metadata":{"id":"JF65AN8eq1cR"},"source":["x2.requires_grad  # we *require* the grad w.r.t. x2, in order to compute the one w.r.t. x..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3rXGqM4XhOyD"},"source":["x2.grad is None  # ...but we had asked Pytorch to only compute the gradient w.r.t. x, so the one wrt x2 is not maintained in memory!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Did you read the user warning up there? That should already give you an intuition of what a _leaf_ tensor is ð!"],"metadata":{"id":"cJM-ZS0V1Wda"}},{"cell_type":"markdown","metadata":{"id":"v6dqtbeo_3BO"},"source":["### Sick of being tracked? ðª\n","\n","You can call `detach()` to **remove a tensor from the computational graph**. This means that the tensor will _not_ be used for computing the gradient and will not partake to the chain rule.\n","\n","We saw one example in the previous notebook, where we were implementing gradient descent by ourselves and we didn't want to compute gradients of the descent steps! Another classical example is when you run a trained model just for inference, which means you already know you won't call `backward()` at all."]},{"cell_type":"code","metadata":{"id":"h-DpCaLcESlq"},"source":["x = torch.tensor(42., requires_grad=True)\n","x2 = x ** 2\n","x2sig = x2\n","print(x2sig.requires_grad)\n","x2nog = x2.detach()\n","print(x2nog.requires_grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Of course, if a tensor is `detach()`ed, a gradient won't be computed for it and thus `requires_grad` will be `False`."],"metadata":{"id":"qE5ODGMpWx5B"}},{"cell_type":"markdown","source":["As a \"blanket solution\", you can also wrap the code block in a context `with torch.no_grad()`. This is equivalent to calling `detach()` everywhere:"],"metadata":{"id":"WRbY27Uu3qJP"}},{"cell_type":"code","metadata":{"id":"4PDmghK0BJqe"},"source":["x = torch.tensor(42., requires_grad=True)\n","x2 = x ** 2\n","print(x2.requires_grad)\n","with torch.no_grad():\n","    x2nog = x ** 2\n","    x3nog = (x2 + 7) ** 3\n","    print(x2nog.requires_grad)\n","    print(x3nog.requires_grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`.no_grad()` is particularly useful for inference, when you are certain that you won't ever call `.backward()`."],"metadata":{"id":"2PgGv86R40Fs"}},{"cell_type":"markdown","metadata":{"id":"pln_4Rgv5Vek"},"source":["Clearly, you won't be able to backpropagate trough a detached tensor because it was removed from the graph:"]},{"cell_type":"code","metadata":{"id":"T3gojG2x5daC"},"source":["try:\n","  x2nog.sum().backward()\n","except Exception as e:\n","  print(e)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dLcCGIFQ5ozR"},"source":["# backward() still works for the tensor that we didn't detach:\n","x2sig.sum().backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ZH_ZFvsUjHx"},"source":["### Tensors ð²\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_jnsJMh1D1aI"},"source":["``torch.Tensor`` is the central class of the `autograd` package.\n","\n","\n","In order to understand in detail how autograd works, it is necessary to dissect some of the most relevant attributes of the Tensors:\n","\n","---\n","\n","- **`data`**:\n","\n","It is the data stored in the tensor. Usually you do not need to access directly this attribute."]},{"cell_type":"code","metadata":{"id":"M0lm9gI0CXMs"},"source":["t = torch.rand(4, 4)\n","t.data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UupbCTeMCWo7"},"source":["\n","---\n","\n","- **`requires_grad`**:\n","\n","  - If `True`, the gradient with respect to this tensor will be computed.\n","  - If `True` and the tensor is a leaf (more on this later!), the gradient will also be saved in the `.grad` attribute.\n","  - If `False`, the gradient with respect to this tensor will _not_ be computed."]},{"cell_type":"code","source":["x = torch.tensor(42., requires_grad=True)\n","x2 = x ** 2\n","x3 = x ** 3\n","f = x2 + x3\n","\n","x.requires_grad, x2.requires_grad, x3.requires_grad, f.requires_grad\n","\n","f.backward()"],"metadata":{"id":"irilpjS-ZI8O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note how all the tensors involved in the computation above have `requires_grad=True`. This means that a gradient will be computed for all of them; these intermediate gradients are all needed by the chain rule, when we will compute `f.backward()`!\n","\n","You can't force any of the intermediate tensors to _not_ have their gradient computed, because this would break the computation of the entire gradient from `f` back to `x`."],"metadata":{"id":"HY2WzdIMblUe"}},{"cell_type":"code","source":["x = torch.tensor(42., requires_grad=True)\n","x2 = x ** 2\n","x3 = x ** 3\n","try:\n","  x3.requires_grad = False\n","except RuntimeError as e:\n","  print(f\"Error: {e}\")\n","f = x2 + x3\n","f.backward()"],"metadata":{"id":"b4_fYQnpcJbh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VaaoNH1aCwDO"},"source":["---\n","\n","- **`grad`**:\n","\n","This attribute is `None` by default; it actually becomes a Tensor when `backward()` is called. The attribute will then contain the computed gradient, and future calls to `backward()` will accumulate (add) gradients into it. Only the leaf nodes of the computational graph with `requires_grad=True` will have the `grad` attribute populated.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uJoahnb9Es0G"},"source":["\n","---\n","\n","- **`grad_fn`**:\n","\n","The backward function that `autograd` will use to use to compute the gradient. For example, if we sum two tensors during the forward pass, then the `grad_fn` attribute of the result will indicate that it was created as a result of an addition operation."]},{"cell_type":"code","metadata":{"id":"XwTMO09rE8Sr"},"source":["t3 = x + x2\n","t3.grad_fn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When we call `backward()` on a tensor, PyTorch will traverse the computational graph from the tensor backward to its inputs, using these `grad_fn` functions to calculate gradients along the way."],"metadata":{"id":"aIP8RYBgF15W"}},{"cell_type":"markdown","metadata":{"id":"qw3kKSuSE7_m"},"source":["\n","---\n","\n","- **`is_leaf`**: a boolean.\n","\n","ð **Only *leaf* tensors with `requires_grad=True` will have their `grad` populated during a call to `backward()`**. To get `grad` populated for non-leaf tensors, you can use `retain_grad()`.\n","Keep in mind that:\n","  - All tensors that have `requires_grad=False` will be leaf tensors by default.\n","  - For tensors that have `requires_grad=True`, they will be leaf tensors if their `grad_fn` is `None`. This means that they are not the result of an operation of tracked tensors, but rather they were created directly by the user."]},{"cell_type":"markdown","source":["**NOTE:** Make sure you are on a GPU runtime before running the following."],"metadata":{"id":"DGspX1szHSjE"}},{"cell_type":"code","metadata":{"id":"0xWCeqfp-A1h"},"source":["a = torch.rand(10, requires_grad=True)\n","a.is_leaf, a.requires_grad"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = torch.rand(10, requires_grad=True) + 2\n","a.is_leaf, a.requires_grad  # was created by the addition operation"],"metadata":{"id":"NnIZtOePHNzc"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BU6DUjE0-PBL"},"source":["a = torch.rand(10, requires_grad=True, device=\"cuda\")\n","a.is_leaf, a.requires_grad  # requires grad, directly created by the user"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-v0q8vXh-NY1"},"source":["a = torch.rand(10).cuda()\n","a.is_leaf, a.requires_grad  # requires_grad=False, thus it is a leaf by default"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = torch.rand(10, requires_grad=True).cuda()\n","a.is_leaf, a.requires_grad  # Was created by the operation that casts a cpu tensor into a cuda tensor.\n","                            # Since we are moving a cpu tensor that requires gradients, this is creating a new version of the tensor in GPU.\n","                            # Therefore 'a' is not a leaf, but the cpu tensor was."],"metadata":{"id":"EOlhAcaHIXPT"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wi8mQKs1-OWY"},"source":["a = torch.rand(10).cuda().requires_grad_()  # Here we move a cpu tensor that does not require gradients, so it stays a leaf, and then modify it.\n","a.is_leaf, a.requires_grad  # requires gradients and has `grad_fn=None`"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NZgZvzGRMv2E"},"source":["a = torch.rand(10, requires_grad=True, device=\"cuda\")\n","b = a + 2                          # non leaf, since requires grad and it is produced by an operation\n","print(b.is_leaf, b.requires_grad)\n","c = b.detach()                     # leaf, it has been detached and now has requires_grad=False\n","print(c.is_leaf, c.requires_grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xRUeDcG1n2oJ"},"source":["---\n","\n","- **`backward()`**:\n","\n","Computes the gradient of current tensor w.r.t. computational graph leaves."]},{"cell_type":"markdown","source":["> ð§  **MEMO**: Remember, the graph is created on the fly during the forward pass, as operations are performed on tensors. When you call `backward()` on the final tensor (usually the rank-0 tensor representing the loss value), Pytorch traverses the computational graph back to the leaf tensors (usually the network parameters), calculating the gradient with respect to them and storing it in their `.grad` attribute."],"metadata":{"id":"knA4ybp1Port"}},{"cell_type":"markdown","metadata":{"id":"apkyGbXEbu9F"},"source":["> ### Leaves recap\n",">\n","> Let's recap the answer to the following question:\n",">\n","> *What are the nodes that will have the `.grad` attribute populated?*\n",">\n","> Here's a computational graph:\n",">\n","> ![](https://raw.githubusercontent.com/erodola/DLAI-s2-2021/main/labs/05/pics/leaves.svg)\n",">\n","> 1. Take the subgraph of nodes with `requires_grad=True` *(green and blue nodes)*\n","> 2. Take the leaves of this subgraph *(green nodes)*\n",">\n","> The nodes selected with this procedure *(green nodes)* will have their `.grad` attribute populated."]},{"cell_type":"markdown","metadata":{"id":"riJ-TeznY5Pr"},"source":["### Gradients\n","\n","Let's look at one last example."]},{"cell_type":"markdown","metadata":{"id":"bm5WbGkZX2om"},"source":["Create a tensor and set ``requires_grad=True`` to track operations:"]},{"cell_type":"code","metadata":{"id":"fgnwModnX_Li"},"source":["x = torch.ones(2, 2, requires_grad=True)\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wC7rds5YYBqF"},"source":["Do some operation:\n"]},{"cell_type":"code","metadata":{"id":"B0fTL53zYIKv"},"source":["y = x + 2\n","y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YwdL-EHbYGZh"},"source":["``y`` was created as a result of a tracked operation, so it has a ``grad_fn``:\n","\n"]},{"cell_type":"code","metadata":{"id":"HJo_kWbbYOJE"},"source":["y.grad_fn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3arTzZIvYQKS"},"source":["Do more operations on `y`:"]},{"cell_type":"code","metadata":{"id":"3KxkovOQYT5Y"},"source":["z = y * y * 3\n","out = z.mean()\n","\n","print(z, out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x0A7egnAY9i2"},"source":["out.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uWAp289WZIpS"},"source":["With this operation we computed $\\frac{\\partial \\, \\text{out}}{\\partial \\, x}$ as well as all the intermediate partial derivatives, but the only one we can actually read is $\\frac{\\partial \\, \\text{out}}{\\partial \\, x}$:\n"]},{"cell_type":"code","metadata":{"id":"L5OxpRZxaSPG"},"source":["x.grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RAO08WUF3fWM"},"source":["Let's double-check why `x.grad` is a `2x2` tensor full of $4.5$.\n","\n","The output is defined as:\n","\n","$$ \\mathrm{out} = \\frac{1}{4} \\sum_i 3(x_i + 2)^2 \\: \\text{ with } x_i = 1 \\, \\forall i$$\n","\n","We have the partial derivatives:\n","\n","$$\n","\\frac{\\partial \\mathrm{out}}{\\partial x_i}\n","= \\frac{3 \\times 2}{4} (x_i + 2)\n","= \\frac{3}{2} (x_i + 2)\n","$$\n","\n","*(Note: the derivative for every $x_j$ with $j \\neq i$ is zero)*\n","\n","\n","Thus, since $x_i=1$ for all $i$ in the input, we obtain $\\frac{\\partial \\mathrm{out}}{\\partial x_i} = \\frac{9}{2} = 4.5$."]},{"cell_type":"markdown","metadata":{"id":"StzZE8g8KNgI"},"source":["##### **EXERCISE**\n","> Understanding if a tensor is a leaf or not is suprisingly tricky, but it is very important to be able to distinguish leaf tensors: **only leaves with `requires_grad=True` tensors will have the grad attribute populated**. The leaves will be the parameters of our neural networks.\n",">\n","> Consider the two following scenarios and try to understand if `a.grad` and/or `b.grad` will be populated.\n",">\n","> **Scenario 1**\n",">\n","> ```python\n","> a = torch.randn(2, 2, requires_grad=True)\n","> b = a ** 2                                \n","> b.requires_grad_(True)                    \n","> b.sum().backward()                        \n","> ```\n","> - [ ] `a.grad` is populated (it is not `None`)\n","> - [ ] `b.grad` is populated (it is not `None`)\n",">\n",">\n","> **Scenario 2**\n",">\n","> ```python\n","> a = torch.randn(2, 2, requires_grad=False)\n","> b = a ** 2                                \n","> b.requires_grad_(True)                    \n","> b.sum().backward()                        \n","> ```\n","> - [ ] `a.grad` is populated (it is not `None`)\n","> - [ ] `b.grad` is populated (it is not `None`)"]},{"cell_type":"code","metadata":{"id":"OVGyQnomGpqG","cellView":"form"},"source":["# @title Solution ð\n","\n","if False:  # Change to true to enable the prints\n","  # 1)\n","  a = torch.randn(2, 2, requires_grad=True)  # leaf tensor that requires grad\n","\n","  b = a ** 2                                 # non leaf tensor: requires grad and produced by an op\n","  b.requires_grad_(True)                     # it already requires a grad!\n","\n","  print(f'a.is_leaf: {a.is_leaf} \\t a.requires_grad: {a.requires_grad}  \\t a.grad_fn: {a.grad_fn}')\n","  print(f'b.is_leaf: {b.is_leaf} \\t b.requires_grad: {b.requires_grad}  \\t b.grad_fn: {b.grad_fn}')\n","\n","  b.sum().backward()                         # just a sample backprop\n","\n","  print(\"\\nGradients:\")\n","  print(f'a.grad: {a.grad}')                 # a is a leaf, thus it will have .grad\n","  print(f'b.grad: {b.grad}')                 # b is not a leaf, thus it will not have .grad\n","\n","  print('\\n\\n---\\n\\n')\n","\n","  # 2)\n","  a = torch.randn(2, 2, requires_grad=False) # leaf tensor that does not requires grad\n","\n","  b = a ** 2                                 # leaf tensor, because not requires grad\n","  b.requires_grad_(True)                     # now it requires a grad and has grad_fn=None! It is a leaf\n","\n","  print(f'a.is_leaf: {a.is_leaf} \\t a.requires_grad: {a.requires_grad}  \\t a.grad_fn: {a.grad_fn}')\n","  print(f'b.is_leaf: {b.is_leaf} \\t b.requires_grad: {b.requires_grad}  \\t b.grad_fn: {b.grad_fn}')\n","\n","  b.sum().backward()                         # just a sample backprop\n","\n","  print(\"\\nGradients:\")\n","  print(f'a.grad: {a.grad}')                 # a is a leaf but does not require grad, thus it will not have .grad\n","  print(f'b.grad: {b.grad}')                 # b is a leaf and requires grad, thus it will have .grad\n","\n","  print('\\n\\n---\\n\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"30r6CUdnjVXq"},"source":["##### **EXERCISE**\n",">\n","> Consider the following expression:\n",">\n","> $$ z = \\frac{\\sqrt{x^2 +1} - \\sqrt{y - 1}}{\\sqrt{x^2 + y^2}} + \\sqrt{y - 1} $$\n",">\n","> Compute the gradients $\\frac{\\partial z}{\\partial x}$, $\\frac{\\partial z}{\\partial y}$, $\\frac{\\partial z}{\\partial \\sqrt{x^2 +1}}$ and $\\frac{\\partial z}{\\partial \\sqrt{y-1}}$ at $x=2$, $y=10$"]},{"cell_type":"code","metadata":{"id":"Qlfee_OzDrM2"},"source":["# Expected results, respectively:\n","# x.grad: 0.08914636820554733\n","# y.grad: 0.15752650797367096\n","# x3.grad: 0.0980580672621727\n","# y2.grad: 0.9019419550895691\n","\n","# âï¸ your solution here\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"desrsm7KWlw2"},"source":["### Autograd Mechanics ð§âð§\n","\n"]},{"cell_type":"markdown","metadata":{"id":"htzNwsSH9_7O"},"source":["#### Custom `Function` ð\n","\n","Remember this example?\n"]},{"cell_type":"code","source":["t = torch.rand(4, 4, requires_grad=True)\n","t2 = torch.rand(4, 4)\n","\n","t3 = t + t2\n","t3.grad_fn"],"metadata":{"id":"AAV-HcBfwoqp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["That `AddBackward0` is an object of the `Function` class. It indicates that `t3` was created by a sum operation, but not only! Together with the `Tensor` class, `Function` makes up the graph that encodes a complete history of computation.\n","\n","All mathematical operations in PyTorch are implemented as objects of the `torch.nn.Autograd.Function` class."],"metadata":{"id":"B61GCHoGw6xO"}},{"cell_type":"markdown","source":["ð **Story time**\n","\n","Once upon a time, we needed to backpropagate through the operation `lambda = eig(X)`, which computes the eigenvalues of a matrix `X`. But the `eig()` operation was not a `Function`! ð±\n","\n","So we implemented our own `Function` and defeated the evil derivative.\n","\n","**Good ending!** Our heroes make their way directly into the sun ð.\n"],"metadata":{"id":"dV1k-llJwnqI"}},{"cell_type":"markdown","source":["Our heroes had to implement these two methods:\n","\n","- `forward()`: the code that performs the operation. It can take as many arguments as you want. All Python objects are accepted as input. _Any input of the `Tensor` type should be explicitly `detach()`ed inside the `forward()` call, so that whatever happens inside the function will not affect the computational graph_; recall that we are going to manually implement the gradient anyway! You can return either a single `Tensor` or a tuple of `Tensor`. Refer to the docs of `Function` to find descriptions of useful methods that can be called only from `forward()`.\n","\n","- `backward()`: gradient formula. The size of its input matches the size of `forward()`'s output. It should return as many `Tensor` s as there were inputs in `forward()`, with each of them containing the gradient w.r.t. its corresponding input. If your inputs didn't require a gradient (`needs_input_grad`, in the `ctx` argument, is a tuple of booleans indicating whether each input needs gradient computation), or were non-Tensor objects, you can return `None`. Also, if you have optional arguments to `forward()` you can return more gradients than there were inputs, as long as they're all `None`.\n","\n","Confused? Let's see an example.\n"],"metadata":{"id":"0PPJADKoAin2"}},{"cell_type":"markdown","source":["We are going to implement our own ReLU from scratch.\n","\n","$$f(x) = \\max \\{0, x \\} $$\n","\n","The _forward_ pass is easy to implement: just write the operation above, and return the result. We'll also need the value of $x$ for computing the derivative $\\frac{\\partial f}{\\partial x}$, so `forward()` must save $x$ for later use.\n","\n","The _backward_ pass is a bit more tricky. Reverse-mode autodiff requires us to compute the _derivative of the **loss** with respect to $x$_:\n","\n","$$ {\\color{blue}{\\frac{\\partial\\ell}{\\partial x}}} = {\\color{green}{\\frac{\\partial \\ell}{\\partial f}}} {\\color{red}{\\frac{\\partial f}{\\partial x}}} $$\n","\n","In particular, `backward()` will receive ${\\color{green}{\\frac{\\partial \\ell}{\\partial f}}}$ as input, and must produce ${\\color{blue}{\\frac{\\partial\\ell}{\\partial x}}}$ in the output. All we must do is compute the portion:\n","\n","$${\\color{red}{\\frac{\\partial f}{ \\partial x}}} =  \\begin{cases} 1 & \\text{if } x > 0\\\\ 0 & \\text{if } x \\le 0 \\end{cases}$$\n","\n","and simply output the product ${\\color{green}{\\frac{\\partial \\ell}{\\partial f}}} {\\color{red}{\\frac{\\partial f}{\\partial x}}}$. Note how, as promised, we are also using $x$ for this calculation.\n","\n"],"metadata":{"id":"4OB-EEqJ0fUX"}},{"cell_type":"code","metadata":{"id":"5YNiK9zZw7JL"},"source":["class MyReLU(torch.autograd.Function):\n","    \"\"\"\n","    We can implement our own custom autograd Functions by subclassing\n","    torch.autograd.Function and implementing the forward and backward passes\n","    which operate on Tensors.\n","    \"\"\"\n","\n","    @staticmethod\n","    def forward(ctx, x):\n","        \"\"\"\n","        In the forward pass we receive a Tensor containing the input and return\n","        a Tensor containing the output. ctx is a context object that can be used\n","        to stash information for backward computation. You can cache arbitrary\n","        objects for use in the backward pass using the ctx.save_for_backward method.\n","        \"\"\"\n","        ctx.save_for_backward(x)\n","\n","        # The operation we do here can be even external to PyTorch, like playing a Marioð¥¸ level and recording the final score.\n","        # We're going simple here: let's implement a standard ReLU.\n","        x_device = x.device\n","        x_dtype = x.dtype\n","        xnumpy = x.cpu().detach().numpy()  # detach() ensures that operations done here do not interfere with the autograd\n","        xnumpy = xnumpy.clip(min=0)\n","\n","        return torch.tensor(xnumpy, dtype=x_dtype, device=x_device)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        \"\"\"\n","        In the backward pass we receive a Tensor containing the gradient of the loss\n","        with respect to the output, and we need to compute the gradient of the loss\n","        with respect to the input.\n","        \"\"\"\n","        input, = ctx.saved_tensors  # unpack the tuple to its only element\n","\n","        grad_input = torch.zeros_like(grad_output)\n","        grad_input[input > 0] = 1\n","        grad_input *= grad_output\n","\n","        # Alternatively, to avoid the element-wise product:\n","        # grad_input = grad_output.clone()  # deep copy\n","        # grad_input[input <= 0] = 0\n","\n","        return grad_input\n","\n","myrelu = MyReLU.apply  # not really needed, but useful to have an alias for future use"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's test this out:"],"metadata":{"id":"d9jLZRig98Dn"}},{"cell_type":"code","metadata":{"id":"VIFmHOz00J1v"},"source":["x = torch.rand(50, requires_grad=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sotv6iQXFZa9"},"source":["out = myrelu(x - 0.5)\n","print(out)  # grad_fn=<MyReLUBackward>\n","out.sum().backward()\n","x.grad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j_WtEDM2FZqU"},"source":["x.grad.zero_()  # usually you should not use this method\n","\n","# -> Let's check our implementation against torch.relu\n","out = torch.relu(x - 0.5)\n","print(out)  # grad_fn=<MyReLUBackward>\n","out.sum().backward()\n","x.grad      # Negative numbers get zeroed, and their grad is zero"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **EXERCISE**\n",">\n","> Implement your own \"ReCU\", defined as:\n",">\n","> $$ f(x) = \\max \\{0, x^3\\} $$\n",">\n","> Write the `forward()` and `backward()` functions, and test them out."],"metadata":{"id":"vJyGzR7j9-yh"}},{"cell_type":"code","source":["# your solution here âï¸\n"],"metadata":{"id":"EYje40FnHVwi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ð Solution\n","\n","class MyReCU(torch.autograd.Function):\n","\n","    @staticmethod\n","    def forward(ctx, x):\n","\n","        ctx.save_for_backward(x)\n","\n","        x_device = x.device\n","        x_dtype = x.dtype\n","        xnumpy = x.cpu().detach().numpy() ** 3\n","        xnumpy = xnumpy.clip(min=0)\n","\n","        return torch.tensor(xnumpy, dtype=x_dtype, device=x_device)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        input, = ctx.saved_tensors\n","        # no cloning necessary, since we are not modifying grad_output directly\n","        grad_input = grad_output * 3 * (input**2) * (input > 0).float()\n","        return grad_input\n","\n","myrecu = MyReCU.apply\n","\n","# testing\n","\n","x = torch.rand(50, requires_grad=True)\n","\n","out = myrecu(10 * x - 5)\n","print(out)\n","\n","out.sum().backward()\n","x.grad"],"metadata":{"cellView":"form","id":"P8lngSXB-UUj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fnFpy_nQ8OVc"},"source":["#### Excluding subgraphs from backward\n","\n","The `requires_grad` flag allows for fine-grained exclusion of subgraphs from gradient computation and can increase efficiency. As a reminder, if any input tensor of an operation has `requires_grad=True`, the output tensor automatically gets `requires_grad=True` as well."]},{"cell_type":"code","metadata":{"id":"6zeW6UGQ8dh1"},"source":["x = torch.randn(5, 5)  # requires_grad=False by default\n","y = torch.randn(5, 5)  # requires_grad=False by default\n","z = torch.randn((5, 5), requires_grad=True)\n","\n","a = x + y\n","b = a + z\n","\n","a.requires_grad, b.requires_grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ciywagdm8vxz"},"source":["Explicitly setting certain tensors to `requires_grad=False` is useful when you want to **âï¸ freeze a subset of parameters of your model** so they are not updated during training. This would be done, for instance, to **finetune** the last layer of a pretrained CNN: simply set `requires_grad=False` for all the parameter tensors except the ones in the last layer.\n","\n","Let's do it:"]},{"cell_type":"code","metadata":{"id":"WKstBTYz81Mc"},"source":["import torchvision\n","model = torchvision.models.resnet18(pretrained=True)  # no need to understand this right now"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZ4f-QrzfpYQ"},"source":["# compute some random prediction from this pretrained network\n","random_prediction = model(torch.rand(2, 3, 224, 224))\n","\n","# dummy loss, just to get some gradients\n","f = random_prediction.sum()\n","\n","# compute the gradients\n","f.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model's parameters, together with the gradient of `f` with respect to them, are stored in..."],"metadata":{"id":"enwFdhlsk-Xh"}},{"cell_type":"code","source":["model.parameters()"],"metadata":{"id":"bRgpZtsdljbJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For example, we can look for all the parameters having a nonzero gradient (based on our dummy loss function):"],"metadata":{"id":"7-jCJH60lnTH"}},{"cell_type":"code","metadata":{"id":"6g8KRjZZe-sI"},"source":["grads = list(x.grad for x in model.parameters() if x.grad.bool().any())\n","len(grads)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's now freeze the pretrained model except for the last layer:"],"metadata":{"id":"NKUefHUgmC3N"}},{"cell_type":"code","metadata":{"id":"MUl9vXuve-ZM"},"source":["# Clear the previous gradients to avoid undue accumulation later\n","model.zero_grad()\n","\n","# Freeze the pretrained model\n","for param in model.parameters():\n","    param.requires_grad = False  # you can do this, because they are all leaves!\n","\n","# Replace the last fully-connected layer\n","# These parameters have requires_grad=True by default\n","model.fc = nn.Linear(512, 100)\n","\n","# Configure an optimizer for the last layer only.\n","# NOTE: we don't actually optimize, this is just to show you how we would setup the training.\n","optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)\n","\n","# Let's compute some gradients\n","random_prediction = model(torch.rand(2, 3, 224, 224))\n","f = random_prediction.sum()\n","f.backward()\n","\n","# Return all grads different than all zeros\n","grads = list(x.grad for x in model.parameters() if x.grad is not None and x.grad.bool().any())\n","len(grads)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nglEF3-I9B0Z"},"source":["#### How autograd encodes the history ð\n","\n","Internally, autograd holds a computational graph of `Function` objects, which can be `apply()` ed to evaluate the result while traversing through the graph. In the forwards pass, autograd simultaneously evaluates the functions and builds populates the graph with objects that will compute the partial derivatives (the `grad_fn` attribute of each `torch.Tensor` is an entry point into this graph). When the forwards pass is completed, we traverse this graph in the backward pass to actually _evaluate_ the gradients.\n","\n","An important thing to note is that **the graph is recreated from scratch at every iteration**, allowing to use arbitrary Python control flow statements, that can change the overall shape and size of the graph at every iteration. You donât have to encode all possible paths before you launch the training - what you run is what you differentiate."]},{"cell_type":"code","metadata":{"id":"BJvT9-ehZcw0"},"source":["x = torch.arange(5, dtype=torch.float, requires_grad=True)\n","y = x - x\n","v = y ** 2\n","z = v + 2\n","z.grad_fn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2LMuCTtAZgxV"},"source":["z.grad_fn.next_functions  # The computational graph is encoded in the grad_fn attributes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cruqnSBVaO64"},"source":["z.grad_fn.next_functions[0][0].next_functions  # If we want, we can manually traverse it"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EeBDi9F2afId"},"source":["z.grad_fn.next_functions[0][0].next_functions[0][0].next_functions  # Leaf tensors have private functions to accumulate grads"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YwIKhBldcKVb"},"source":["# After the leaf tensors, we don't compute anything!\n","# Even if there are other leaf tensors in the computational graph behind those (that do not require a grad)\n","z.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vt6ZCLWl9aIG"},"source":["#### In-place operations with autograd\n","\n","From our discussion so far, you might suppose that in-place operations on Pytorch tensors can potentially **overwrite values required to compute gradients**. This is true: with an in-place operation, we may break the backpropagation mechanism.\n","\n","Here's an example:\n","\n","```python\n","x = torch.rand(5, requires_grad=True)\n","y = x * 2\n","y.add_(torch.sqrt(y * x))\n","```\n","\n","What happens to the internal attributes of `y` as we keep overwriting it?\n","\n","Each in-place operation actually rewrites the computational graph. This can be tricky, especially if there are many `Tensors` that reference the same storage (e.g. created by indexing or transposing), and in-place functions will actually raise an error if the storage of modified inputs is referenced by any other `Tensor`. In contrast, **out-of-place versions simply allocate new objects and keep references** to the old graph."]},{"cell_type":"markdown","metadata":{"id":"zejrsiY79wnT"},"source":["##### In-place correctness checks ð\n","\n","Every tensor keeps a _version counter_, incremented each time the tensor is marked as \"dirty\" by an in-place operation. When a `Function` uses `save_for_backward()` to save references of any tensors for its backward pass, a version counter of their containing `Tensor` is saved as well. Once you access `self.saved_tensors`, the version is checked. If it is greater than the saved value, an error is raised. This ensures that if youâre using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct."]},{"cell_type":"code","metadata":{"id":"4ZSN9j7uNhUF"},"source":["x = torch.rand(10, requires_grad=True)\n","o = x * 10\n","o.retain_grad()\n","o2 = o + 10\n","o2.retain_grad()\n","y = torch.rand(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dOoIoXCbONCY"},"source":["o._version  # the version counter is initialized to zero"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uhC3kaUyOBKu"},"source":["o.add_(-1)  # dirty modify, increase the version counter\n","o._version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cb2vqqAfOBVs"},"source":["z = x + y  # it does not modify x in place\n","x._version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yGWUFDNgOBbg"},"source":["x = x + x  # x is a new tensor\n","x._version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DRrF9bMSKNTz"},"source":["# ð Let's break autodiff with in-place operations\n","\n","try:\n","  x = torch.ones(5, requires_grad=True)\n","  x2 = (x + 1).sqrt()\n","  z = (x2 - 10)\n","  x2[0] = -1\n","  z.sum().backward()\n","except Exception as e:\n","  print(e)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3kVgPlPH7nug"},"source":["References:\n","\n","- [PyTorch docs](https://pytorch.org/docs/stable/index.html)\n","- [Autograd tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n","- [Autograd mechanics](https://pytorch.org/docs/stable/notes/autograd.html)\n","- [Extending PyTorch](https://pytorch.org/docs/stable/notes/extending.html)\n","- Nice [blogpost](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)\n","- Nice [blogpost](https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95) number  two"]},{"cell_type":"markdown","metadata":{"id":"TL_gZXtj0oWD"},"source":["\n","## JAX: functional differentiation ð\n","\n","The approach used by PyTorch or Tensorflow is essentially the same, apart from small differences in how they handle the computational graph. However, _there are alternatives_.\n","\n","We will not introduce [**Jax**](https://jax.readthedocs.io/en/latest/) in these notebooks, but it deserves a mention of honor because it employs a fundamentally different technique to perform autodiff:\n","\n","![](https://sjmielke.com/images/blog/jax-purify/banner.png)"]},{"cell_type":"markdown","metadata":{"id":"Xwkalx3GSpmG"},"source":["# The `torch.nn` package\n"]},{"cell_type":"markdown","source":["\n","PyTorch provides the elegantly designed modules and classes\n","[`torch.nn`](https://pytorch.org/docs/stable/nn.html),\n","[`torch.optim`](https://pytorch.org/docs/stable/optim.html),\n","[`Dataset`](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset),\n","and [`DataLoader`](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)\n","to help you create and train neural networks.\n","You have already seen how to use `torch.optim`, `Dataset` and `DataLoader`. In this section we will review all these classes together with the new `torch.nn` package to understand how they work together to simplify our life.\n","\n","To develop this understanding, we will first train a basic neural net on the MNIST dataset _without_ using any of these modules: we will just use the most basic PyTorch tensor functionality.\n","\n","---\n","\n","Our final goal is to reach an elegant, general structure suitable for most problems and models with minor tweaks:\n","\n","```python\n","# load data\n","# instantiate model\n","# instantiate optimizer\n","\n","# for each epoch:\n","  # train the model on the training set\n","  # evaluate the model on one or more evaluation sets\n","  # log metrics (e.g. log, accuracy)\n","```"],"metadata":{"id":"7gNTB1k5_zEn"}},{"cell_type":"markdown","metadata":{"id":"CG-o5SyW-2Gt"},"source":["\n","### Classifying *all* handwritten digits\n","\n","\n","Last time we used MNIST, we considered the binary classification of ones and sevens. This time we'll consider all classes, leading to _multinomial_ logistic regression problem. This is also known as _softmax regression_.\n","\n","#### One-hot encodings\n","\n","We will represent the class information using the *one-hot* representation (aka indicator vectors):\n","\n","$$0 = (1,0,0,0,0,0,0,0,0,0) $$\n","$$1 = (0,1,0,0,0,0,0,0,0,0) $$\n","$$...$$\n","$$9 = (0,0,0,0,0,0,0,0,0,1) $$\n","\n","The output of our model will thus be a 10-dimensional vector, which we can interpret as a probability distribution. For example:\n","\n","$$(0,0,0.98,0.01,0,0,0,0,0.01,0) $$\n","\n","In this example, the model predicts that the given input is a 2 with 98\\% probability, a 3 with 1% probability, and an 8 with 1%.\n","\n","#### Softmax\n","\n","In standard logistic regression, the predicted value $p=\\sigma(\\mathbf{w}^\\top \\mathbf{x}+b)$ was easy\n","to interpret as a probability, thanks to the squashing between $[0,1]$ performed by the sigmoid $\\sigma()$.\n","\n","What is the analogous of the sigmoid for more than one dimension? It's called _softmax_:\n","\n","$$\\text{softmax}(\\mathbf{x}) = \\{\\frac{\\exp(x_0)}{\\sum_{j}^{ }\\exp(x_j))}, \\frac{\\exp(x_1)}{\\sum_{j}^{ }\\exp(x_j))}, ... , \\frac{\\exp(x_9)}{\\sum_{j}^{ }\\exp(x_j))}\\}$$\n","\n","By construction, $\\text{softmax}(\\mathbf{x})$ sums up to one.\n","\n","If we use $\\text{softmax}$ instead of $\\sigma$, we have a multinomial logistic model yielding a distribution $\\mathbf{p}$ instead of a single value $p$. However, as we'll explain below, we can output $\\log(\\mathbf{p})$ instead of $\\mathbf{p}$ to make things simpler, and we can also generalize the cross-entropy loss to the multinomial case.\n","\n","Let's start by gaining an intuition of how softmax behaves; we'll do this by considering $\\exp(\\alpha x)$ with different $\\alpha$:"]},{"cell_type":"code","source":["x = torch.rand(40)"],"metadata":{"id":"GEAjeJAeAUjQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Softmax: crank up the alpha!  { run: \"auto\" }\n","\n","import plotly.graph_objects as go\n","\n","alpha = 47  #@param {type:\"slider\", min:1, max:50, step:1}\n","\n","sx = torch.exp(alpha*x)\n","sx /= sx.sum()\n","\n","fig = go.Figure()\n","# fig.add_trace(go.Bar(y=x, name='x', marker_color='blue'))\n","fig.add_trace(go.Bar(y=sx, name='sx', marker_color='red'))\n","fig.update_layout(barmode='group', title='Softmax of a random vector', width=800, height=300)\n","fig.show()"],"metadata":{"cellView":"form","id":"y1KsQqs49snF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You got an idea if why it's called soft**max**? One useful way to think about the softmax is as a smooth approximation of the indicator function."],"metadata":{"id":"pl3IiuM_BNN0"}},{"cell_type":"markdown","metadata":{"id":"8e4yna-oB4hm"},"source":["Now let's prepare for our first neural model. For this example, *we will avoid the use of the ready-made MNIST dataset in the `torchvision` package* but we will manually build it, to understand how to adapt the concepts on other datasets not available in `torchvision`.\n","\n","This MNIST dataset is in numpy array format.\n","\n"]},{"cell_type":"code","metadata":{"id":"VQFl7YCmqTtl"},"source":["!wget https://s3.amazonaws.com/img-datasets/mnist.npz\n","\n","def load_data_impl():\n","    # file retrieved by:\n","    #   wget https://s3.amazonaws.com/img-datasets/mnist.npz -O code/dlgo/nn/mnist.npz\n","    # code based on:\n","    #   site-packages/keras/datasets/mnist.py\n","    path = 'mnist.npz'\n","    f = np.load(path)\n","    x_train, y_train = f['x_train'].reshape(-1, 784), f['y_train']\n","    x_test, y_test = f['x_test'].reshape(-1, 784), f['y_test']\n","    f.close()\n","    return (x_train.astype(np.float32), y_train), (x_test.astype(np.float32), y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z9044-BwqnlF"},"source":["(x_train, y_train), (x_valid, y_valid) = load_data_impl()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Sd-OMv57q_0"},"source":["> **EXERCISE**: We almost always want to normalize datasets, i.e. we _subtract_ their mean, and _scale_ by their standard deviation. Compute the mean and standard deviation of the MNIST dataset and then normalize it."]},{"cell_type":"code","metadata":{"id":"iJOefQyF7o-H"},"source":["# Normalization with pre-computed values\n","\n","x_train = (x_train / 255 - 0.13) / 0.3  # data normalization\n","x_valid = (x_valid / 255 - 0.13) / 0.3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"27rmdvyLCCCz"},"source":["Each image is `28 x 28`, and is being stored as a flattened row of length\n","`784 (=28x28)`. Let's take a look at one; we need to reshape it to 2d\n","first.\n","\n"]},{"cell_type":"code","metadata":{"id":"dbMuZbkyCUM3"},"source":["import plotly.express as px\n","import numpy as np\n","\n","print(x_train.shape)\n","px.imshow(x_train[0].reshape((28, 28)), color_continuous_scale='gray')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qaC7cIBuCuz1"},"source":["PyTorch uses ``torch.tensor``, rather than numpy arrays, so we need to\n","convert our data:"]},{"cell_type":"code","metadata":{"id":"x8WbOmgCC-aL"},"source":["import torch\n","\n","x_train, y_train, x_valid, y_valid = map(\n","  torch.tensor, (x_train, y_train, x_valid, y_valid)\n",")\n","n, c = x_train.shape\n","y_train = y_train.long()  # pytorch wants int64 as indices\n","y_valid = y_valid.long()\n","print(x_train, y_train)\n","print(x_train.shape)\n","print(y_train.min(), y_train.max())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_osaceZPDLNq"},"source":["### Neural net from scratch (no torch.nn)\n","\n","Our first neural model is built using nothing but PyTorch tensor operations.\n","\n","For the weights, we set `requires_grad` **after** the initialization, since we don't want the initialization function to be included in the gradient computation. (remember that a trailling `_` in PyTorch means that the operation is performed in-place.)\n","\n","We are initializing the weights with a simplified version of\n","[Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), i.e. by multiplying with $\\frac{1}{\\sqrt{n}}$\n"]},{"cell_type":"code","metadata":{"id":"QBGRtDolDOb7"},"source":["import math\n","\n","weights = torch.randn(784, 10) / math.sqrt(784)  # Xavier init.\n","weights.requires_grad_()                         # Start to track the weights\n","bias = torch.zeros(10, requires_grad=True)       # Initialize the bias with zeros"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Model\n","\n","How did we choose the shape of the `weights` tensor? Let's recall the good old binary logistic regression model:\n","\n","$$ p = \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b) $$\n","\n","where $\\mathbf{x}_i$ is a flattened image and $\\mathbf{w}$ is a vector of weights parametrizing our model. This means that we have one weight per pixel, in total $28\\cdot 28=784$ weights.\n","\n","In the multinomial case, we now have:\n","\n","$$ \\mathbf{p} = \\text{softmax} (\\mathbf{W}^\\top \\mathbf{x}_i + \\mathbf{b}) \\,.$$\n","\n","Since $\\mathbf{p}$ represents the predicted probabilities over 10 classes, matrix $\\mathbf{W}$ must be $784 \\times 10$.\n","\n","In fact, a more numerically stable approach is to ask our model to output the log-probabilities instead of the probabilities, namely:\n","\n","\\begin{align}\n","\\log(\\mathbf{p}) &= \\log(\\text{softmax} (\\mathbf{W}^\\top \\mathbf{x}_i + \\mathbf{b}) )\\\\\n","&=(\\mathbf{W}^\\top \\mathbf{x}_i + \\mathbf{b})- \\log ( \\sum_{j=0}^9 \\exp( \\mathbf{W}^\\top \\mathbf{x}_i + \\mathbf{b} ) )\n","\\end{align}"],"metadata":{"id":"E-Vi7c9LWGrY"}},{"cell_type":"markdown","metadata":{"id":"1pMn2kbHEQ6X"},"source":["Seems complex, but it's easy! Here's our implementation of the model:"]},{"cell_type":"code","metadata":{"id":"5wqmVvHDEkL6"},"source":["def log_softmax(x):\n","  return x - x.exp().sum(-1).log().unsqueeze(-1)\n","\n","def model(xb):\n","  return log_softmax(xb @ weights + bias)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PjilYcEBEtrt"},"source":["The `model()` function will take a mini-batch of data as input. Let's test it:"]},{"cell_type":"code","metadata":{"id":"SUg_hARSE2ag"},"source":["bs = 64  # batch size (number of images)\n","\n","xb = x_train[0:bs]  # a mini-batch from x\n","preds = model(xb)  # log predictions\n","preds[0], preds.shape\n","print(torch.exp(preds[0]), preds.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Loss and accuracy\n","\n","Of course, we don't expect these predictions to be any better than random, since we are still using our model with random weights. You know what we must do: **define a loss**, and update the model weights iteratively to minimize it."],"metadata":{"id":"9XhuJS5Ify9q"}},{"cell_type":"markdown","metadata":{"id":"stbCgSOvE_ay"},"source":["> **EXERCISE**: The loss.\n",">\n","> Implement the **negative log-likelihood** (NLL) loss, given a batch of log probabilities (`input`) and the corresponding ground-truth labels (`target`).\n",">\n","> Mathematically, for an image $\\mathbf{x}_i$ with true class label $c$, this loss is defined as:\n",">\n","> $$ \\ell(\\mathbf{x}_i) = -\\log (p_{c_i}) $$\n",">\n","> where $p_{c_i}$ is the model's predicted probability that $\\mathbf{x}_i$ belongs to class $c$.\n",">\n","> The loss makes sense: higher probabilities for the correct class yield lower losses.\n",">\n","> For a batch of $N$ images, simply average their NLL loss to make it independent from the batch size:\n",">\n","> $$ \\ell(\\mathbf{x}_i) = - \\frac{1}{N} \\sum_i \\log (p_{c_i}) $$"]},{"cell_type":"code","metadata":{"id":"RnG-h7RTAlft"},"source":["def nll(input, target):\n","    # your code here âï¸  -- can you do it without a for loop?\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f6Xn8kDiyPrd","cellView":"form"},"source":["# @title Solution ð\n","\n","def nll(input, target):\n","    loss = -torch.gather(input, 1, target[:, None]).mean()\n","    return loss\n","\n","loss_func = nll"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i8ejxEPXFDeu"},"source":["Let's check the loss of our random model, so we can see if we improve\n","after a backprop pass later.\n"]},{"cell_type":"code","metadata":{"id":"6of3BzPHFHFa"},"source":["yb = y_train[0:bs]\n","print(loss_func(preds, yb))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6rpLY6LfFLEe"},"source":["Let's also implement a function to calculate the **accuracy** of our model. For each prediction, if the largest probability is at the same index as the ground-truth target, then the prediction was correct:"]},{"cell_type":"code","metadata":{"id":"0eJzoBiUFM4E"},"source":["def accuracy(out, yb):\n","  preds = torch.argmax(out, dim=1)\n","  return (preds == yb).float().mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u6Sc0bnVFP4N"},"source":["Let's check the accuracy of our random model, so we can see if our\n","accuracy improves as our loss improves:"]},{"cell_type":"code","metadata":{"id":"IbvoeUNbFSRG"},"source":["print(accuracy(preds, yb))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **EXERCISE**: What is the maximum possible accuracy?"],"metadata":{"id":"R1QWncfo8aKi"}},{"cell_type":"markdown","metadata":{"id":"nUPbYZMUFTv9"},"source":["#### Training\n","\n","We can now run a training loop.  For each iteration, we will:\n","\n","- select a mini-batch of data (of size ``bs``)\n","- use the model to make predictions\n","- calculate the loss\n","- ``loss.backward()`` computes the gradient of the loss w.r.t. to `weights` and `bias`.\n","- use the gradients to update weights and bias\n","\n","When we update the weights and bias, we must remember using the `torch.no_grad()` context manager to avoid that the update steps themselves will be tracked for the next calculation of the gradient.\n","\n","Also, we must set the gradients to zero before the next iteration, otherwise they will be accumulated across all the iterations."]},{"cell_type":"markdown","source":["ð **Debugging**: Unfortunately, Colab only provides the built-in debugger available in `IPython.core.debugger`. Uncomment ``set_trace()`` below to try it out. [Here](https://nblock.org/2011/11/15/pdb-cheatsheet/) you can find a cheatsheet of the pdb commands.\n","\n","Alternatively, you can fire up your favorite IDE; the debugger integrated in [PyCharm](https://www.jetbrains.com/pycharm/) is convenient, as the pro license is free for students."],"metadata":{"id":"dn18ezEM-KP5"}},{"cell_type":"code","metadata":{"id":"tl12s4guFRVP"},"source":["from IPython.core.debugger import set_trace\n","\n","lr = 0.05  # learning rate\n","epochs = 5\n","\n","for epoch in range(epochs):\n","  for i in range((n - 1) // bs + 1):\n","    start_i = i * bs\n","    # set_trace()\n","\n","    end_i = start_i + bs\n","    xb = x_train[start_i:end_i]\n","    yb = y_train[start_i:end_i]\n","    pred = model(xb)\n","    loss = loss_func(pred, yb)\n","    loss.backward()\n","\n","    with torch.no_grad():\n","      weights -= weights.grad * lr\n","      bias -= bias.grad * lr\n","      weights.grad.zero_()\n","      bias.grad.zero_()\n","\n","  print(f\"epoch: {epoch}, training loss: {loss:.2f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HcQiC9qwKLn8"},"source":["That's it: we've created and trained a minimal neural network with just one layer entirely from scratch! ð\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YfL4-S6YKzjH"},"source":["### Refactor: use `torch.nn.functional`\n","\n","\n","We will now refactor our code, so that it does the same thing as before, only\n","we'll start taking advantage of PyTorch's ``nn`` classes to make it more concise\n","and flexible.\n","\n","First step: let's use `torch.nn.functional`'s loss and activation functions. Currently we are using the log-softmax activation, and the negative log-likelihood loss. PyTorch does all in one: `F.cross_entropy`.\n"]},{"cell_type":"code","metadata":{"id":"E8mrfSyALH7N"},"source":["import torch.nn.functional as F\n","\n","loss_func = F.cross_entropy\n","\n","def model(xb):\n","  return xb @ weights + bias  # we don't explicitly apply log-softmax anymore"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can still use the same training loop code from before, where the `model` and `loss_func` have been redefined."],"metadata":{"id":"Ws4Wj30vIHKA"}},{"cell_type":"markdown","metadata":{"id":"VmFJ7hRMMFRL"},"source":["### Refactor: use `nn.Module`\n","\n","Next up, we'll use ``nn.Module`` and ``nn.Parameter``, for a clearer and more\n","concise training loop. We subclass ``nn.Module`` to create a class that\n","holds our weights, bias, and method for the forward step.  ``nn.Module`` has a\n","number of attributes and methods (such as ``.parameters()`` and ``.zero_grad()``)\n","which we will be using."]},{"cell_type":"code","metadata":{"id":"L2dUPFzqOKJQ"},"source":["from torch import nn\n","\n","class Mnist_Logistic(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n","    self.bias = nn.Parameter(torch.zeros(10))\n","\n","  def forward(self, xb):\n","    return xb @ self.weights + self.bias"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GjgeOoIIOK8Y"},"source":["Since we're now using an object instead of just using a function (our old `model()`), we first have to instantiate our model:\n","\n"]},{"cell_type":"code","metadata":{"id":"QW-63GBxOOrM"},"source":["model = Mnist_Logistic()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ro2IHPQBOSu-"},"source":["Now we can calculate the loss in the same way as before. Note that\n","``nn.Module`` objects are used as if they are functions (i.e they are\n","*callable*), but behind the scenes Pytorch will call our ``forward``\n","method automatically.\n","\n","> The `__call__` method of the Modules, internally calls the `forward` method and *does other stuff* (e.g. registers some hooks, you can check the implementation [here](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module)). Thus, you should always call the forward with `model(inputs)` and never directly `model.forward(inputs)`."]},{"cell_type":"code","metadata":{"id":"Wf2E-zpGOVeq"},"source":["print(loss_func(model(xb), yb))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qZIQ7m6zOWkp"},"source":["Previously in our training loop we had to update `weights` and `bias` explicitly and manually zero out the grads, like this:\n","\n","```python\n","  with torch.no_grad():\n","      weights -= weights.grad * lr\n","      bias -= bias.grad * lr\n","      weights.grad.zero_()\n","      bias.grad.zero_()\n","```\n","\n","Now we can take advantage of `model.parameters()` and `model.zero_grad()` (which are both defined by PyTorch for ``nn.Module``) to make these steps more concise and less prone to error:\n","\n","```python\n","  with torch.no_grad():\n","      for p in model.parameters(): p -= p.grad * lr\n","      model.zero_grad()\n","```\n","\n","We'll wrap our little training loop in a ``fit`` function so we can run it\n","again later.\n","\n"]},{"cell_type":"code","metadata":{"id":"yXXDTXEcOmnG"},"source":["def fit():\n","  for epoch in range(epochs):\n","    for i in range((n - 1) // bs + 1):\n","      start_i = i * bs\n","      end_i = start_i + bs\n","      xb = x_train[start_i:end_i]\n","      yb = y_train[start_i:end_i]\n","      pred = model(xb)\n","      loss = loss_func(pred, yb)\n","\n","      loss.backward()\n","      with torch.no_grad():\n","        for p in model.parameters():\n","          p -= p.grad * lr\n","        model.zero_grad()\n","\n","fit()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eYYnuadIOsim"},"source":["Let's double-check that our loss has gone down:"]},{"cell_type":"code","metadata":{"id":"2QLTIeS_Opya"},"source":["print(loss_func(model(xb), yb))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8s550An7OwDj"},"source":["### Refactor: use `nn.Linear`\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MiDLxIksO6IU"},"source":["We continue to refactor our code.  Instead of manually defining and\n","initializing ``self.weights`` and ``self.bias``, and calculating ``xb  @\n","self.weights + self.bias``, we will instead use the Pytorch class\n","[`nn.Linear`](https://pytorch.org/docs/stable/nn.html#linear-layers) for a\n","linear layer, which does all that for us.\n","\n","Pytorch has many predefined layers that can greatly simplify our code, and often making it faster too."]},{"cell_type":"code","metadata":{"id":"tS-fO2BUPCnk"},"source":["class Mnist_Logistic(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.lin = nn.Linear(784, 10)\n","\n","  def forward(self, xb):\n","    return self.lin(xb)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wcP82sxdPC-_"},"source":["We instantiate our model and calculate the loss in the same way as before:"]},{"cell_type":"code","metadata":{"id":"RqX0CbrhPFHE"},"source":["model = Mnist_Logistic()\n","print(loss_func(model(xb), yb))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_mwPhCfSPGFW"},"source":["We are still able to use our same ``fit`` method as before:"]},{"cell_type":"code","metadata":{"id":"kC_0w6jgPJAo"},"source":["fit()\n","\n","print(loss_func(model(xb), yb))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pIYrW4yJPKNG"},"source":["### Refactor: use `torch.optim`\n","\n","We already explored the `torch.optim` package in the *Logistic Regression and Optimization* notebook: obviously we can use optimizers to train our models!\n","\n","We can use the ``step`` method from our optimizer to take a forward step, instead\n","of manually updating each parameter.\n","\n","This will let us replace our previous custom optimization step:\n","\n","```python\n","with torch.no_grad():\n","  for p in model.parameters(): p -= p.grad * lr\n","  model.zero_grad()\n","```\n","\n","and instead use just:\n","```python\n","opt.step()\n","opt.zero_grad()\n","```\n","where `opt` can be any fancy optimizer."]},{"cell_type":"code","metadata":{"id":"3LQLPJOtPVMe"},"source":["from torch import optim"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jr80yLuWP5qa"},"source":["We'll define a little function to create our model and optimizer so we\n","can reuse it in the future:"]},{"cell_type":"code","metadata":{"id":"p0-cis3PP7ih"},"source":["def get_model():\n","  model = Mnist_Logistic()\n","  return model, optim.SGD(model.parameters(), lr=lr)\n","\n","model, opt = get_model()\n","print(loss_func(model(xb), yb))\n","\n","for epoch in range(epochs):\n","  for i in range((n - 1) // bs + 1):\n","    start_i = i * bs\n","    end_i = start_i + bs\n","    xb = x_train[start_i:end_i]\n","    yb = y_train[start_i:end_i]\n","    pred = model(xb)\n","    loss = loss_func(pred, yb)\n","\n","    loss.backward()\n","    opt.step()\n","    opt.zero_grad()\n","\n","print(loss_func(model(xb), yb))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that we are **not replacing the training loop**. The optimizer only helps us for a single step; we still have to run the training loop ourselves."],"metadata":{"id":"wUkm0SdXLm39"}},{"cell_type":"markdown","metadata":{"id":"G7O345TQQBDx"},"source":["### Refactor: use `Dataset`\n","\n","\n","We already explored the PyTorch abstract Dataset class in the *Linear models and Pytorch Datasets* notebook. PyTorch's [`TensorDataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset)\n","is a `Dataset` that wraps tensors. This also gives us a way to easily iterate, index, and slice the dataset with a more compact code."]},{"cell_type":"code","metadata":{"id":"dj-cZyjdRE48"},"source":["from torch.utils.data import TensorDataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dmy9HEaKRFSk"},"source":["Both ``x_train`` and ``y_train`` can be combined in a single ``TensorDataset``,\n","which will be easier to iterate over and slice:\n","\n"]},{"cell_type":"code","metadata":{"id":"da2JI5n7RHhW"},"source":["train_ds = TensorDataset(x_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dVWAfKjlRN_1"},"source":["Previously, we had to iterate through minibatches of x and y values separately:\n","\n","```python\n","    xb = x_train[i*bs : i*bs+bs]\n","    yb = y_train[i*bs : i*bs+bs]\n","```\n","\n","Now, we can do these two steps together:\n","\n","```python\n","    xb, yb = train_ds[i*bs : i*bs+bs]\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"K4C3_c3nRc_B"},"source":["model, opt = get_model()\n","\n","for epoch in range(epochs):\n","  for i in range((n - 1) // bs + 1):\n","    xb, yb = train_ds[i * bs: i * bs + bs]\n","    pred = model(xb)\n","    loss = loss_func(pred, yb)\n","\n","    loss.backward()\n","    opt.step()\n","    opt.zero_grad()\n","\n","print(loss_func(model(xb), yb))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EGh9MSe9RgRh"},"source":["### Refactor: use `DataLoader`\n","\n","We already explored the PyTorch abstract `DataLoader` class in the *Linear models and Pytorch Datasets* notebook.  \n","\n","Remember that among other things Pytorch's ``DataLoader`` can provide us with minibatches automatically.\n"]},{"cell_type":"code","metadata":{"id":"mxdaL230Ty0i"},"source":["from torch.utils.data import DataLoader\n","\n","train_ds = TensorDataset(x_train, y_train)\n","train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KIydhfapTzjg"},"source":["Previously, our loop iterated over batches `xb, yb` like this:\n","\n","```python\n","for i in range((n-1)//bs + 1):\n","  xb,yb = train_ds[i*bs : i*bs+bs]\n","  pred = model(xb)\n","```\n","\n","Now, our loop is much cleaner, as `xb, yb` are loaded automatically from the data loader:\n","\n","```python\n","for xb, yb in train_dl:\n","  pred = model(xb)\n","```\n"]},{"cell_type":"code","metadata":{"id":"Zskh-TRNT_vl"},"source":["model, opt = get_model()\n","\n","for epoch in range(epochs):\n","  for xb, yb in train_dl:\n","    pred = model(xb)\n","    loss = loss_func(pred, yb)\n","\n","    loss.backward()\n","    opt.step()\n","    opt.zero_grad()\n","\n","print(loss_func(model(xb), yb))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OhXNbbPaUGKs"},"source":["Thanks to Pytorch's ``nn.Module``, ``nn.Parameter``, ``Dataset``, and ``DataLoader``,\n","our training loop is now dramatically smaller and easier to understand. Let's\n","now try to add the basic features necessary to create effecive models in practice.\n"]},{"cell_type":"markdown","metadata":{"id":"jT_IS5bbUI83"},"source":["### Add: validation set\n","\n","Although we have neglected it so far for simplicity, you should **always**  have\n","a [`validation set`](https://www.fast.ai/2017/11/13/validation-sets/) to identify overfitting.\n","\n","**A note about shuffling:** Shuffling the training data is important to prevent correlation between batches and overfitting. On the other hand, the validation loss will be identical whether we shuffle the validation set or not. Since shuffling takes extra time and makes qualitative comparisons more difficult, _it makes no sense to shuffle the validation data_.\n","\n","Still, we'll build mini-batches for the validation set as well, for efficiency reasons (e.g. avoid a memory bottleneck of loading the entire validation set at once). We'll use a batch size for the validation set that is twice as large as\n","that for the training set, because it doesn't need to\n","store any gradients.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"gf1C5uehV5vN"},"source":["train_ds = TensorDataset(x_train, y_train)\n","train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n","\n","valid_ds = TensorDataset(x_valid, y_valid)\n","valid_dl = DataLoader(valid_ds, batch_size=bs * 2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WlTH6IbNV6Mt"},"source":["We will calculate and print the validation loss at the end of each epoch.\n","\n","In the code below, we also call `model.train()` before training, and `model.eval()` before inference; this will be required by layers such as ``nn.BatchNorm2d``\n","and ``nn.Dropout`` to ensure appropriate behavior, and it's good practice to do this always to be safe."]},{"cell_type":"code","metadata":{"id":"NOr_UhEWWMwB"},"source":["model, opt = get_model()\n","\n","for epoch in range(epochs):\n","\n","  model.train()\n","  for xb, yb in train_dl:\n","    pred = model(xb)\n","    loss = loss_func(pred, yb)\n","\n","    loss.backward()\n","    opt.step()\n","    opt.zero_grad()\n","\n","  model.eval()\n","  with torch.no_grad():\n","    valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl) / len(valid_dl)\n","\n","  print(epoch, valid_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Is the loss always going down? Try a few times!"],"metadata":{"id":"dcxliWu2Ove1"}},{"cell_type":"markdown","metadata":{"id":"BEyPjw9iWIbS"},"source":["### Add: fit() and get_data()\n","\n","We'll now do a little refactoring of our own. Since we go through a similar\n","process twice of calculating the loss for both the training set and the\n","validation set, let's make that into its own function, ``step``, which\n","computes the loss for one batch and possibly one optimization step.\n","\n","We pass an optimizer in for the training set, and use it to perform\n","backprop.  For the validation set, we don't pass an optimizer, so the\n","method doesn't perform backprop."]},{"cell_type":"code","metadata":{"id":"9SRe9AWJWmp8"},"source":["def step(model, loss_func, xb, yb, opt=None):\n","  loss = loss_func(model(xb), yb)\n","\n","  if opt is not None:\n","    loss.backward()\n","    opt.step()\n","    opt.zero_grad()\n","\n","  return loss.item(), len(xb)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g8bL45LWW1BV"},"source":["``fit`` runs the necessary operations to train our model and compute the\n","training and validation losses for each epoch:"]},{"cell_type":"code","metadata":{"id":"8lEyMuQlW2ox"},"source":["import numpy as np\n","\n","def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n","  for epoch in range(epochs):\n","    model.train()\n","    for xb, yb in train_dl:\n","      step(model, loss_func, xb, yb, opt)\n","\n","    model.eval()\n","    with torch.no_grad():\n","      losses, batch_sizes = zip(\n","        *[step(model, loss_func, xb, yb) for xb, yb in valid_dl]\n","      )\n","    val_loss = np.sum(np.multiply(losses, batch_sizes)) / np.sum(batch_sizes)\n","\n","    # Here's how zip() works:\n","    # - step() returns (loss.item(), len(xb)) for each batch.\n","    # - the input to zip() is [(loss1, size1), (loss2, size2), ...].\n","    # - the output is two tuples: (loss1, loss2, ...) and (size1, size2, ...).\n","\n","    print(epoch, val_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZnssBXcdW84R"},"source":["``get_data`` returns dataloaders for the training and validation sets."]},{"cell_type":"code","metadata":{"id":"vogWj8_aW9wr"},"source":["def get_data(train_ds, valid_ds, bs):\n","  return (\n","    DataLoader(train_ds, batch_size=bs, shuffle=True),\n","    DataLoader(valid_ds, batch_size=bs * 2),\n","  )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_QVYlPUUW_Vk"},"source":["Now, our whole process of obtaining the data loaders and fitting the\n","model can be run in 3 lines of code:"]},{"cell_type":"code","metadata":{"id":"1N-0Pqf5XB9m"},"source":["train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n","model, opt = get_model()\n","fit(epochs, model, loss_func, opt, train_dl, valid_dl)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aLM0YU-QD1jq"},"source":["You can use these basic 3 lines of code to train a wide variety of models.\n","Let's see if we can use them to train a convolutional neural network (CNN, you will learn about them in the next lessons)!\n"]},{"cell_type":"markdown","metadata":{"id":"kaJphSnyXDp6"},"source":["### Our first CNN\n","\n","We are now going to build our neural network with three convolutional layers.\n","Because none of the functions in the previous section assume anything about\n","the model form, we'll be able to use them to train a CNN without any modification.\n","\n","We will use Pytorch's predefined [`Conv2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d) class\n","as our convolutional layer. We define a CNN with 3 convolutional layers.\n","Each convolution is followed by a ReLU.  At the end, we perform an\n","average pooling.\n","\n"]},{"cell_type":"code","metadata":{"id":"BEUjxfJJYGaa"},"source":["class Mnist_CNN(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n","    self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n","    self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n","\n","  def forward(self, xb):\n","    xb = xb.view(-1, 1, 28, 28) # conv layers expect input with shape (batch size, channels, height, width)\n","    xb = F.relu(self.conv1(xb))\n","    xb = F.relu(self.conv2(xb))\n","    xb = F.relu(self.conv3(xb))\n","    xb = F.avg_pool2d(xb, 4)\n","    return xb.view(-1, xb.size(1)) # reshape to 2d tensor:\n","                                   # first dimension is batch size (calculated automatically) .\n","                                   # second dimension is whatever the channel dimension xb.size(1)\n","                                   # has become after the convolutions and pooling.\n","\n","lr = 0.1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NQCB2rTFYH1V"},"source":["model = Mnist_CNN()\n","opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n","\n","fit(epochs, model, loss_func, opt, train_dl, valid_dl)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nMdlDhLCYQzW"},"source":["### Use: nn.Sequential\n","\n","``torch.nn`` has another handy class we can use to simplify our code: [`Sequential`](https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential).\n","\n","A ``Sequential`` object runs each of the modules contained within it, in a\n","sequential manner. This is a simpler way of writing a neural network.\n","\n","Also, this is especially useful when we want to easily define a\n","**custom layer** that wraps around a given function. For instance, PyTorch doesn't have a `view` layer. We are going to create one for our network.\n","\n","The class we define below, called `Lambda`, will create a layer that we can then use to construct a network with `Sequential`."]},{"cell_type":"code","metadata":{"id":"knRC2zytaLHR"},"source":["class Lambda(nn.Module):\n","  def __init__(self, func):\n","    super().__init__()\n","    self.func = func\n","\n","  def forward(self, x):\n","    return self.func(x)\n","\n","\n","def preprocess(x):\n","  return x.view(-1, 1, 28, 28)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Zns3VyIaL_w"},"source":["Instead of using the module `Mnist_CNN`, we can now build our network by using ``Sequential``:"]},{"cell_type":"code","metadata":{"id":"PEwvSOXBaOSu"},"source":["model = nn.Sequential(\n","  Lambda(preprocess),  # prepare the images for the conv layers\n","  nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n","  nn.ReLU(),\n","  nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n","  nn.ReLU(),\n","  nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n","  nn.ReLU(),\n","  nn.AvgPool2d(4),\n","  Lambda(lambda x: x.view(x.size(0), -1)),\n",")\n","\n","opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n","\n","fit(epochs, model, loss_func, opt, train_dl, valid_dl)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"INMixcSqaPOs"},"source":["### Use: DataLoader wrappers\n","\n","Our CNN is fairly concise, but it only works with MNIST, because:\n"," - It assumes the input is a 28\\*28 long vector (look at the reshaping done with `Lambda(preprocess)`)\n"," - It assumes that the final CNN grid size is 4\\*4 (since that's the average\n","pooling kernel size we used)\n","\n","Let's get rid of these two assumptions, so our model works with any 2d\n","single channel image. First, we can remove the initial Lambda layer,\n","moving the data preprocessing directly into the data loader:\n","\n"]},{"cell_type":"code","metadata":{"id":"XK2YR60Zayt0"},"source":["def preprocess(x, y):\n","    return x.view(-1, 1, 28, 28), y\n","\n","\n","class WrappedDataLoader:\n","  def __init__(self, dl, func):\n","    self.dl = dl\n","    self.func = func\n","\n","  def __len__(self):\n","    return len(self.dl)\n","\n","  # By using yield, we turn the __iter__ method into a generator,\n","  # making it lazily evaluate and only process batches as needed:\n","\n","  def __iter__(self):\n","    batches = iter(self.dl)\n","    for b in batches:\n","      yield (self.func(*b))  # func() reshapes the images, *b unpacks the batch\n","\n","train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n","train_dl = WrappedDataLoader(train_dl, preprocess)\n","valid_dl = WrappedDataLoader(valid_dl, preprocess)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qe7Bs1qRa3u6"},"source":["Next, we can replace ``nn.AvgPool2d`` with ``nn.AdaptiveAvgPool2d``, which\n","allows us to define the size of the *output* tensor we want, rather than\n","the *input* tensor we have. As a result, our model will work with inputs of any size."]},{"cell_type":"code","metadata":{"id":"r82O48g8a6GB"},"source":["model = nn.Sequential(\n","  nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n","  nn.ReLU(),\n","  nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n","  nn.ReLU(),\n","  nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n","  nn.ReLU(),\n","  nn.AdaptiveAvgPool2d(1),\n","  Lambda(lambda x: x.view(x.size(0), -1)),\n",")\n","\n","opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oIisgg_Ga_zT"},"source":["Let's try it out:"]},{"cell_type":"code","metadata":{"id":"jGNTZnI9bCBT"},"source":["fit(epochs, model, loss_func, opt, train_dl, valid_dl)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F7FZjuoT6Z9E"},"source":["### Use: your GPU\n","\n","If you're lucky enough to have access to a CUDA-capable GPU, you can\n","use it to speed up your code. First check that your GPU is working in\n","Pytorch:\n","\n"]},{"cell_type":"code","metadata":{"id":"GApMyXwe6Z9F"},"source":["print(torch.cuda.is_available())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lmZr8ET_6Z9G"},"source":["And then create a device object for it:\n","\n"]},{"cell_type":"code","metadata":{"id":"E7YAOdXh6Z9H"},"source":["dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jhEnbsNO6Z9M"},"source":["Let's update `preprocess` to move batches to the GPU:"]},{"cell_type":"code","metadata":{"id":"Kc84KuON6Z9M"},"source":["def preprocess(x, y):\n","    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n","\n","train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n","train_dl = WrappedDataLoader(train_dl, preprocess)\n","valid_dl = WrappedDataLoader(valid_dl, preprocess)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J6FQIUUy6Z9O"},"source":["Finally, we can move our model to the GPU.\n","\n"]},{"cell_type":"code","metadata":{"id":"plAiM1d76Z9O"},"source":["model.to(dev)\n","opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7kbRyLkW6Z9Q"},"source":["It runs faster now!\n","\n"]},{"cell_type":"code","metadata":{"id":"Q8SEkJq-6Z9Q"},"source":["fit(epochs, model, loss_func, opt, train_dl, valid_dl)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TZqlt4gxb4IK"},"source":["### Closing thoughts\n","\n","\n","We now have a general data pipeline and training loop that you can use for\n","training many types of models using Pytorch.\n","\n","Of course, there are many things you'll want to add, such as data augmentation, hyperparameter tuning, monitoring training, transfer learning, and so forth.\n","\n","Let's summarize what we've seen:\n","\n"," - **torch.nn**\n","\n","   + ``Module``: creates a callable that behaves like a function, but can also\n","     contain a state. It knows what ``Parameter``s it\n","     contains and can zero all their gradients, loop through them for weight updates, etc.\n","   + ``Parameter``: a wrapper for a tensor that tells a ``Module`` that it has weights\n","     that need updating during backprop. Only tensors with the `requires_grad` attribute set are updated.\n","   + ``functional``: a module (usually imported into the ``F`` namespace by convention) containing activation functions, loss functions, etc, as well as non-stateful\n","     versions of layers such as convolutional and linear layers.\n"," - ``torch.optim``: Contains optimizers such as ``SGD``, which update the weights\n","   of ``Parameter`` during the backward step.\n"," - ``Dataset``: An abstract interface of objects with a ``__len__`` and a ``__getitem__``,\n","   including classes provided with Pytorch such as ``TensorDataset``.\n"," - ``DataLoader``: Takes any ``Dataset`` and creates an iterator that returns batches of data.\n","\n","---\n","\n","Tutorial on `torch.nn` adapted from this [tutorial](https://pytorch.org/tutorials/beginner/nn_tutorial.html).\n"]},{"cell_type":"markdown","metadata":{"id":"F-hWBkrccW0a"},"source":["## Playground: MLPs expressivity\n"]},{"cell_type":"markdown","metadata":{"id":"ybGgvfMg4tY1"},"source":["What type of functions does a MLP learn?\n","\n","To help our intuition let's try to visualize the *functions that a deep mlp learns*, while changing some hyperparameters.\n","\n","In order to do that, we will use this simple parametric model, where the dimension of the intermediate tensors (the **hidden dimension**), the **number of layers** and the **type of activation** are parametric."]},{"cell_type":"markdown","source":["â ï¸ **Compatibility warning:** For this last plot we are downgrading to plotly 5.11.0."],"metadata":{"id":"TeaT2DBLqSDs"}},{"cell_type":"code","source":["!pip install plotly==5.11.0"],"metadata":{"id":"ILFHz11OqTUv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import plotly.express as px\n","import numpy as np\n","import torch.nn.functional as F\n","from typing import Mapping, Union, Optional, Callable\n","import numpy as np\n","import argparse\n","import torch.optim as optim\n","import plotly.graph_objects as go\n","from torchvision import datasets, transforms\n","from tqdm.notebook import tqdm"],"metadata":{"id":"IZk7YN8sqwPU"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vAuX8Xs70L7N"},"source":["class MLP2D(nn.Module):\n","  def __init__(self,\n","               num_layers: int,\n","               hidden_dim: int,\n","               activation: Callable[[torch.Tensor], torch.Tensor]\n","               ) -> None:\n","    super().__init__()\n","\n","    self.first_layer = nn.Linear(in_features=2,\n","                                 out_features=hidden_dim)\n","\n","    self.layers = nn.ModuleList()  # A list of modules: automatically exposes nested parameters to optimize.\n","                                   # Parameters contained in a normal python list are not returned by model.parameters()\n","    for i in range(num_layers):\n","      self.layers.append(\n","          nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n","      )\n","    self.activation = activation\n","\n","    self.last_layer = nn.Linear(in_features=hidden_dim,\n","                                out_features=1)\n","\n","\n","  def forward(self, meshgrid: torch.Tensor) -> torch.Tensor:\n","    \"\"\"\n","    Applies transformations to each (x, y) independently\n","\n","    :param meshgrid: tensor of dimensions [..., 2], where ... means any number of dims\n","    \"\"\"\n","    out = meshgrid\n","\n","    out = self.first_layer(out)  # First linear layer, transforms the hidden dimensions from 2 (the coordinates) to `hidden_dim`\n","    for layer in self.layers:    # Apply `k` (linear, activation) layer\n","      out = layer(out)\n","      out = self.activation(out)\n","    out = self.last_layer(out)   # Last linear layer to bring the `hiddem_dim` features back to the 2 coordinates x, y\n","\n","    return out.squeeze(-1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iuigWCUw0Fn6"},"source":["Let's try this model to understand how it works:"]},{"cell_type":"code","metadata":{"id":"ZkM_bnG805Ij"},"source":["model = MLP2D(num_layers=3,\n","              hidden_dim=10,\n","              activation=torch.nn.functional.relu)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yifV8sgr1VNj"},"source":["model.parameters  # Explore the parameters, i.e. the trainable tensors that require a grad in this model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b6BiIFh54UfB"},"source":["Let's try to execute this model on some input:"]},{"cell_type":"code","metadata":{"id":"hx3W5pM_1M64"},"source":["model(torch.as_tensor([1., 2.]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sOUMV2VR2F-7"},"source":["model(torch.as_tensor([[1., 2.]]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VkvFMQ9l2IV_"},"source":["model(torch.as_tensor([[1., 2.], [1., 2.]]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Lyv636M4FUS"},"source":["model(torch.rand(2, 3, 2, 2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nmiH2ZiA3AMG"},"source":["Now we are going to sample points from some simple function, to create our dataset. Our model will try to reconstruct the function from these points.\n","\n","We can use a `Dataset` to keep the code clean and organize the sampled points.\n","Keep in mind, for each point the model will receive the $(x, y)$ coordinates and try to predict the $z = f(x, y)$ coordinate."]},{"cell_type":"code","metadata":{"id":"Dt7y2LnqEdBP"},"source":["from typing import Dict\n","\n","class PointsDataset(torch.utils.data.Dataset):\n","  def __init__(self, x: torch.Tensor, y_true: torch.Tensor) -> None:\n","    super().__init__()\n","\n","    self.x = x\n","    self.y_true = y_true\n","\n","  def __len__(self) -> int:\n","    return self.y_true.shape[0]\n","\n","  def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n","    return {                    # For the idx-th sample\n","        'x': self.x[idx, ...],  # the \"x\" are the (x, y) coordinates of the idx-th point\n","        'y': self.y_true[idx]   # the \"y\" is  the (z) coordinate of the idx-th point\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_COgcGBmBMb","cellView":"form"},"source":["# @title Dataset creation { run: \"auto\" }\n","\n","fn_names = {\n","    'peaks': peaks,\n","    'rastrigin': rastrigin,\n","    'rosenbrock': rosenbrock,\n","    'simple_fn': simple_fn,\n","    'simple_fn2': simple_fn2\n","}\n","\n","base_fn = 'peaks' #@param [\"simple_fn\", \"simple_fn2\", \"peaks\",  \"rastrigin\", \"rosenbrock\"]\n","n_rand = 80 #@param {type:\"slider\", min:0, max:200, step:1}\n","plot_lim = 3  #@param {type:\"slider\", min:0, max:50, step:1}\n","plot_height = 700 #@param {type:\"slider\", min:0, max:1500, step:50}\n","\n","lim = plot_lim\n","fn = fn_names[base_fn]\n","\n","x_random = torch.rand(n_rand) * lim * 2 - lim\n","y_random = torch.rand(n_rand) * lim * 2 - lim\n","\n","xy_data = torch.stack((x_random, y_random), dim=-1)\n","xy_groundtruth = fn(xy_data)\n","\n","\n","points_dl =  torch.utils.data.DataLoader(\n","    PointsDataset(x=xy_data, y_true=xy_groundtruth),\n","    batch_size=16,\n","    shuffle=True\n",")\n","\n","points = torch.cat((xy_data, xy_groundtruth[..., None]), dim=-1)\n","\n","\n","plot_points_over_landscape(fn, points, lim=lim, height=plot_height)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hsa46rL_TNkU","cellView":"form"},"source":["# @title Learned function from sampled points { run: \"auto\" }\n","\n","activation_names = {\n","    'relu': torch.relu,\n","    'leaky_relu': torch.nn.functional.leaky_relu,\n","    'elu': torch.nn.functional.elu,\n","    'sigmoid': torch.sigmoid,\n","    'tanh': torch.tanh,\n","}\n","\n","num_layers = 2 #@param {type:\"slider\", min:0, max:20, step:1}\n","\n","activation_fn = 'relu' #@param [\"relu\", \"leaky_relu\", \"elu\", \"sigmoid\",  \"tanh\"]\n","activation_fn = activation_names[activation_fn]\n","\n","hidden_dim = 16 #@param {type:\"slider\", min:0, max:512, step:1}\n","\n","num_epochs = 610 #@param {type:\"slider\", min:0, max:1000, step:10}\n","learning_rate = 0.001 #@param {type:\"number\"}\n","\n","plot_lim = 4  #@param {type:\"slider\", min:0, max:50, step:1}\n","plot_height = 900 #@param {type:\"slider\", min:0, max:1500, step:50}\n","\n","def energy(y_pred, y_true):\n","  return torch.nn.functional.mse_loss(y_pred, y_true)\n","\n","\n","model = MLP2D(num_layers=num_layers, activation=activation_fn, hidden_dim=hidden_dim)\n","opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = model.to(device)\n","\n","for i in tqdm(range(num_epochs), desc=\"epoch\"):\n","    for batch in points_dl:\n","      x = batch['x'].to(device)\n","      y = batch['y'].to(device)\n","      y_pred = model(x)\n","\n","      loss = energy(y_pred, y)\n","\n","      loss.backward()\n","      opt.step()\n","      opt.zero_grad()\n","\n","\n","plot_points_over_landscape(model.cpu(), points.cpu(), lim=lim, height=plot_height).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["!!!\n","\n","If you have problems visualizing the previous animation in Google Colab, please explore it in this streamlit app:\n","\n","-  https://lucmos-demo-nn-expressivity-ui-plhjjk.streamlit.app/"],"metadata":{"id":"NNhux_Irj4YH"}}]}